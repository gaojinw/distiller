{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import traceback\n",
    "from collections import OrderedDict, defaultdict\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchnet.meter as tnt\n",
    "# script_dir = os.path.dirname(__file__)\n",
    "script_dir = os.path.abspath('/host/model_compression/distiller/examples/style_transfer_compression')\n",
    "module_path = os.path.abspath(os.path.join(script_dir, '..', '..'))\n",
    "try:\n",
    "    import distiller\n",
    "except ImportError:\n",
    "    sys.path.append(module_path)\n",
    "    import distiller\n",
    "import apputils\n",
    "from distiller.data_loggers import *\n",
    "import distiller.quantization as quantization\n",
    "from models import ALL_MODEL_NAMES, create_model\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam\n",
    "import re\n",
    "\n",
    "sys.path.append('/host/model_compression/distiller/examples/style_transfer_compression/network')\n",
    "import utils\n",
    "from transformer_net import *\n",
    "from vgg import *\n",
    "\n",
    "# Logger handle\n",
    "msglogger = None\n",
    "\n",
    "\n",
    "def float_range(val_str):\n",
    "    val = float(val_str)\n",
    "    if val < 0 or val >= 1:\n",
    "        raise argparse.ArgumentTypeError('Must be >= 0 and < 1 (received {0})'.format(val_str))\n",
    "    return val\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Distiller image classification model compression')\n",
    "parser.add_argument('--train-dataset', metavar='train_DIR', help='path to training dataset')\n",
    "parser.add_argument('--val-dataset', metavar='val_DIR', help='path to validation dataset')\n",
    "#parser.add_argument('--arch', '-a', metavar='ARCH', default='resnet18',\n",
    "#                    choices=ALL_MODEL_NAMES,\n",
    "#                    help='model architecture: ' +\n",
    "#                    ' | '.join(ALL_MODEL_NAMES) +\n",
    "#                    ' (default: resnet18)')\n",
    "#parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "#                    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('--epochs', default=90, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('-b', '--batch-size', default=256, type=int,\n",
    "                    metavar='N', help='mini-batch size (default: 256)')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,\n",
    "                    metavar='LR', help='initial learning rate')\n",
    "#parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
    "#                    help='momentum')\n",
    "#parser.add_argument('--weight-decay', '--wd', default=1e-4, type=float,\n",
    "#                    metavar='W', help='weight decay (default: 1e-4)')\n",
    "parser.add_argument('--print-freq', '-p', default=10, type=int,\n",
    "                    metavar='N', help='print frequency (default: 10)')\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--evaluate', dest='evaluate', action='store_true',\n",
    "                    help='evaluate model on validation set')\n",
    "parser.add_argument('--pretrained', default='', type=str, metavar='PretrainedPath',\n",
    "                    help='use pre-trained model')\n",
    "parser.add_argument('--act-stats', dest='activation_stats', choices=[\"train\", \"valid\", \"test\"], default=None,\n",
    "                    help='collect activation statistics (WARNING: this slows down training)')\n",
    "parser.add_argument('--masks-sparsity', dest='masks_sparsity', action='store_true', default=False,\n",
    "                    help='print masks sparsity table at end of each epoch')\n",
    "parser.add_argument('--param-hist', dest='log_params_histograms', action='store_true', default=False,\n",
    "                    help='log the paramter tensors histograms to file (WARNING: this can use significant disk space)')\n",
    "SUMMARY_CHOICES = ['sparsity', 'compute', 'model', 'modules', 'png', 'png_w_params', 'onnx']\n",
    "parser.add_argument('--summary', type=str, choices=SUMMARY_CHOICES,\n",
    "                    help='print a summary of the model, and exit - options: ' +\n",
    "                    ' | '.join(SUMMARY_CHOICES))\n",
    "parser.add_argument('--compress', dest='compress', type=str, nargs='?', action='store',\n",
    "                    help='configuration file for pruning the model (default is to use hard-coded schedule)')\n",
    "parser.add_argument('--sense', dest='sensitivity', choices=['element', 'filter', 'channel'],\n",
    "                    help='test the sensitivity of layers to pruning')\n",
    "parser.add_argument('--extras', default=None, type=str,\n",
    "                    help='file with extra configuration information')\n",
    "parser.add_argument('--deterministic', '--det', action='store_true',\n",
    "                    help='Ensure deterministic execution for re-producible results.')\n",
    "parser.add_argument('--gpus', metavar='DEV_ID', default=None,\n",
    "                    help='Comma-separated list of GPU device IDs to be used (default is to use all available devices)')\n",
    "parser.add_argument('--name', '-n', metavar='NAME', default=None, help='Experiment name')\n",
    "parser.add_argument('--out-dir', '-o', dest='output_dir', default='logs', help='Path to dump logs and checkpoints')\n",
    "parser.add_argument('--validation-size', '--vs', type=float_range, default=0.1,\n",
    "                    help='Portion of training dataset to set aside for validation')\n",
    "parser.add_argument('--adc', dest='ADC', action='store_true', help='temp HACK')\n",
    "parser.add_argument('--adc-params', dest='ADC_params', default=None, help='temp HACK')\n",
    "parser.add_argument('--confusion', dest='display_confusion', default=False, action='store_true',\n",
    "                    help='Display the confusion matrix')\n",
    "parser.add_argument('--earlyexit_lossweights', type=float, nargs='*', dest='earlyexit_lossweights', default=None,\n",
    "                    help='List of loss weights for early exits (e.g. --lossweights 0.1 0.3)')\n",
    "parser.add_argument('--earlyexit_thresholds', type=float, nargs='*', dest='earlyexit_thresholds', default=None,\n",
    "                    help='List of EarlyExit thresholds (e.g. --earlyexit 1.2 0.9)')\n",
    "parser.add_argument('--num-best-scores', dest='num_best_scores', default=1, type=int,\n",
    "                    help='number of best scores to track and report (default: 1)')\n",
    "parser.add_argument('--load-serialized', dest='load_serialized', action='store_true', default=False,\n",
    "                    help='Load a model without DataParallel wrapping it')\n",
    "\n",
    "quant_group = parser.add_argument_group('Arguments controlling quantization at evaluation time'\n",
    "                                        '(\"post-training quantization)')\n",
    "quant_group.add_argument('--quantize-eval', '--qe', action='store_true',\n",
    "                         help='Apply linear-symmetric quantization to model before evaluation. Applicable only if'\n",
    "                              '--evaluate is also set')\n",
    "quant_group.add_argument('--qe-bits-acts', '--qeba', type=int, default=8, metavar='NUM_BITS',\n",
    "                         help='Number of bits for quantization of activations')\n",
    "quant_group.add_argument('--qe-bits-wts', '--qebw', type=int, default=8, metavar='NUM_BITS',\n",
    "                         help='Number of bits for quantization of weights')\n",
    "quant_group.add_argument('--qe-bits-accum', type=int, default=32, metavar='NUM_BITS',\n",
    "                         help='Number of bits for quantization of the accumulator')\n",
    "quant_group.add_argument('--qe-clip-acts', '--qeca', action='store_true',\n",
    "                         help='Enable clipping of activations using max-abs-value averaging over batch')\n",
    "quant_group.add_argument('--qe-no-clip-layers', '--qencl', type=str, nargs='+', metavar='LAYER_NAME', default=[],\n",
    "                         help='List of fully-qualified layer names for which not to clip activations. Applicable'\n",
    "                              'only if --qe-clip-acts is also set')\n",
    "parser.add_argument(\"--image-size\", type=int, default=256,\n",
    "                    help=\"size of training images, default is 256 X 256\")\n",
    "parser.add_argument(\"--style-size\", type=int, default=None,\n",
    "                    help=\"size of style-image, default is the original size of style image\")\n",
    "parser.add_argument('--content-weight', default=1e5, type=float,\n",
    "                    metavar='CW', help='Content Weights')\n",
    "parser.add_argument('--style-weight', default=1e10, type=float,\n",
    "                    metavar='SW', help='Style Weights')\n",
    "parser.add_argument('--style-image', default='', type=str, metavar='StyleImagePath',\n",
    "                    help='Style Image Path')\n",
    "\n",
    "distiller.knowledge_distillation.add_distillation_args(parser, ALL_MODEL_NAMES, True)\n",
    "\n",
    "\n",
    "def check_pytorch_version():\n",
    "    if torch.__version__ < '0.4.0':\n",
    "        print(\"\\nNOTICE:\")\n",
    "        print(\"The Distiller \\'master\\' branch now requires at least PyTorch version 0.4.0 due to \"\n",
    "              \"PyTorch API changes which are not backward-compatible.\\n\"\n",
    "              \"Please install PyTorch 0.4.0 or its derivative.\\n\"\n",
    "              \"If you are using a virtual environment, do not forget to update it:\\n\"\n",
    "              \"  1. Deactivate the old environment\\n\"\n",
    "              \"  2. Install the new environment\\n\"\n",
    "              \"  3. Activate the new environment\")\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "def create_activation_stats_collectors(model, collection_phase):\n",
    "    \"\"\"Create objects that collect activation statistics.\n",
    "\n",
    "    This is a utility function that creates two collectors:\n",
    "    1. Fine-grade sparsity levels of the activations\n",
    "    2. L1-magnitude of each of the activation channels\n",
    "\n",
    "    Args:\n",
    "        model - the model on which we want to collect statistics\n",
    "        phase - the statistics collection phase which is either \"train\" (for training),\n",
    "                or \"valid\" (for validation)\n",
    "\n",
    "    WARNING! Enabling activation statsitics collection will significantly slow down training!\n",
    "    \"\"\"\n",
    "    class missingdict(dict):\n",
    "        \"\"\"This is a little trick to prevent KeyError\"\"\"\n",
    "        def __missing__(self, key):\n",
    "            return None  # note, does *not* set self[key] - we don't want defaultdict's behavior\n",
    "\n",
    "    distiller.utils.assign_layer_fq_names(model)\n",
    "\n",
    "    activations_collectors = {\"train\": missingdict(), \"valid\": missingdict(), \"test\": missingdict()}\n",
    "    if collection_phase is None:\n",
    "        return activations_collectors\n",
    "    collectors = missingdict()\n",
    "    collectors[\"sparsity\"] = SummaryActivationStatsCollector(model, \"sparsity\", distiller.utils.sparsity)\n",
    "    collectors[\"l1_channels\"] = SummaryActivationStatsCollector(model, \"l1_channels\",\n",
    "                                                                distiller.utils.activation_channels_l1)\n",
    "    collectors[\"apoz_channels\"] = SummaryActivationStatsCollector(model, \"apoz_channels\",\n",
    "                                                                  distiller.utils.activation_channels_apoz)\n",
    "    collectors[\"records\"] = RecordsActivationStatsCollector(model, classes=[torch.nn.Conv2d])\n",
    "    activations_collectors[collection_phase] = collectors\n",
    "    return activations_collectors\n",
    "\n",
    "\n",
    "def save_collectors_data(collectors, directory):\n",
    "    \"\"\"Utility function that saves all activation statistics to Excel workbooks\n",
    "    \"\"\"\n",
    "    for name, collector in collectors.items():\n",
    "        collector.to_xlsx(os.path.join(directory, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "class Object(object):\n",
    "    pass\n",
    "    \n",
    "args = Object()\n",
    "args.train_dataset = '/host/dataset/COCO/train'\n",
    "args.val_dataset = '/host/dataset/COCO/val'\n",
    "args.gpus='1'\n",
    "args.epochs = 12\n",
    "args.batch_size = 8\n",
    "args.print_freq = 500\n",
    "args.output_dir = './logs'\n",
    "args.name = None\n",
    "args.deterministic = False\n",
    "args.cuda = 1\n",
    "args.earlyexit_thresholds = None\n",
    "args.resume = None\n",
    "args.lr = 1e-4\n",
    "args.style_size = None\n",
    "args.log_interval = 2\n",
    "args.log_params_histograms = False\n",
    "args.activation_stats = False\n",
    "args.ADC = False\n",
    "args.image_size = 256\n",
    "args.pretrained = 'None'\n",
    "args.content_weight = 1e5\n",
    "args.style_weight = 1e10\n",
    "args.style_image = './pretrained/images/style_images/style4.jpg'\n",
    "args.compress = None # 'GradientRankedFilterPruner_v1.yaml'\n",
    "args.num_best_scores = 1\n",
    "args.masks_sparsity = False\n",
    "args.display_confusion = False\n",
    "args.kd_teacher = False\n",
    "print(args.resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  496.9888,    68.8403,   113.4070,    -9.2730, -1388.0399,\n",
      "         -424.3785,   618.0903,  -366.4390,   116.2851,   -32.3259,\n",
      "         -889.9984, -1031.1152,  -591.8314,  -368.0196,  1433.5139,\n",
      "         -118.2308,    -8.7417,     7.4077,  1460.9594, -1125.3129,\n",
      "          496.6891,  -426.6752,  -140.7191, -1055.2037,    -9.5175,\n",
      "          -29.9717,  -337.8140,   381.2503,  -222.6381, -1694.3574,\n",
      "          -63.7828,     6.3086], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "for name, layer in model.named_parameters():\n",
    "    if name == 'conv1.conv2d.weight':\n",
    "        print(layer.grad[:,0,0,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log file for this run: /host/model_compression/distiller/examples/style_transfer_compression/logs/2018.11.22-061822/2018.11.22-061822.log\n",
      "Distiller: 0.3.0-pre\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Object object at 0x7f6aac2fda90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizer Type: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer Args: {'lr': 0.0001, 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'betas': (0.9, 0.999)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------\n",
      "Logging to TensorBoard - remember to execute the server:\n",
      "> tensorboard --logdir='./logs'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "\ttraining=82783\n",
      "\n",
      "Created masker for parameter conv1.conv2d.weight\n",
      "Created masker for parameter conv1.conv2d.bias\n",
      "Created masker for parameter in1.weight\n",
      "Created masker for parameter in1.bias\n",
      "Created masker for parameter conv2.conv2d.weight\n",
      "Created masker for parameter conv2.conv2d.bias\n",
      "Created masker for parameter in2.weight\n",
      "Created masker for parameter in2.bias\n",
      "Created masker for parameter conv3.conv2d.weight\n",
      "Created masker for parameter conv3.conv2d.bias\n",
      "Created masker for parameter in3.weight\n",
      "Created masker for parameter in3.bias\n",
      "Created masker for parameter res1.conv1.conv2d.weight\n",
      "Created masker for parameter res1.conv1.conv2d.bias\n",
      "Created masker for parameter res1.in1.weight\n",
      "Created masker for parameter res1.in1.bias\n",
      "Created masker for parameter res1.conv2.conv2d.weight\n",
      "Created masker for parameter res1.conv2.conv2d.bias\n",
      "Created masker for parameter res1.in2.weight\n",
      "Created masker for parameter res1.in2.bias\n",
      "Created masker for parameter res2.conv1.conv2d.weight\n",
      "Created masker for parameter res2.conv1.conv2d.bias\n",
      "Created masker for parameter res2.in1.weight\n",
      "Created masker for parameter res2.in1.bias\n",
      "Created masker for parameter res2.conv2.conv2d.weight\n",
      "Created masker for parameter res2.conv2.conv2d.bias\n",
      "Created masker for parameter res2.in2.weight\n",
      "Created masker for parameter res2.in2.bias\n",
      "Created masker for parameter res3.conv1.conv2d.weight\n",
      "Created masker for parameter res3.conv1.conv2d.bias\n",
      "Created masker for parameter res3.in1.weight\n",
      "Created masker for parameter res3.in1.bias\n",
      "Created masker for parameter res3.conv2.conv2d.weight\n",
      "Created masker for parameter res3.conv2.conv2d.bias\n",
      "Created masker for parameter res3.in2.weight\n",
      "Created masker for parameter res3.in2.bias\n",
      "Created masker for parameter res4.conv1.conv2d.weight\n",
      "Created masker for parameter res4.conv1.conv2d.bias\n",
      "Created masker for parameter res4.in1.weight\n",
      "Created masker for parameter res4.in1.bias\n",
      "Created masker for parameter res4.conv2.conv2d.weight\n",
      "Created masker for parameter res4.conv2.conv2d.bias\n",
      "Created masker for parameter res4.in2.weight\n",
      "Created masker for parameter res4.in2.bias\n",
      "Created masker for parameter res5.conv1.conv2d.weight\n",
      "Created masker for parameter res5.conv1.conv2d.bias\n",
      "Created masker for parameter res5.in1.weight\n",
      "Created masker for parameter res5.in1.bias\n",
      "Created masker for parameter res5.conv2.conv2d.weight\n",
      "Created masker for parameter res5.conv2.conv2d.bias\n",
      "Created masker for parameter res5.in2.weight\n",
      "Created masker for parameter res5.in2.bias\n",
      "Created masker for parameter deconv1.conv2d.weight\n",
      "Created masker for parameter deconv1.conv2d.bias\n",
      "Created masker for parameter in4.weight\n",
      "Created masker for parameter in4.bias\n",
      "Created masker for parameter deconv2.conv2d.weight\n",
      "Created masker for parameter deconv2.conv2d.bias\n",
      "Created masker for parameter in5.weight\n",
      "Created masker for parameter in5.bias\n",
      "Created masker for parameter deconv3.conv2d.weight\n",
      "Created masker for parameter deconv3.conv2d.bias\n",
      "Warmup: 5000 samples (8 per mini-batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  -91.8549,   170.6449,    35.3226,   168.5001,  -409.3874,\n",
      "          133.2486,   627.6133,  -886.0362,  -942.3327,   -28.5606,\n",
      "         -446.6496,   109.3673,   149.0329,  -771.4691,   637.1243,\n",
      "          173.0287,   -87.3631,     4.6201,  2303.1438, -1383.0386,\n",
      "          822.1470,  -421.7875,   -57.0181,  -476.6962,    11.6445,\n",
      "          -13.2139, -1015.7404,  -295.1177,    31.4592, -2219.3477,\n",
      "         -317.2803,   499.8040], device='cuda:1')\n",
      "tensor([  192.1691,   721.4189,     4.2501,   -65.3367, -1005.8824,\n",
      "          -14.7701,  1709.9736, -1262.0632, -1637.7439,    -2.5499,\n",
      "        -1314.8909,  -575.9225,  -249.5179, -2629.4287,  3023.8303,\n",
      "          636.8612,  -321.0949,     4.5012,  2304.2803,  -713.6462,\n",
      "         1608.8020,  -668.3934,  -338.3925, -1566.9121,    24.1716,\n",
      "          -50.6935, -1311.4360,  -367.8708,  -253.8996, -5041.5132,\n",
      "         -565.0148,   625.4479], device='cuda:1')\n",
      "tensor([ 1297.0695,   603.4865,   284.7516,   -66.9628, -1571.2848,\n",
      "        -1692.6957,  2674.1572, -1385.4841,  1061.7869,   -38.5127,\n",
      "        -2587.6472, -1540.1958, -2228.7122, -2576.0479,  5757.6572,\n",
      "          837.7618,  -283.8711,    17.8500,  2468.0447, -6042.7471,\n",
      "          773.1490, -1252.6973,  -496.7089, -3390.5225,    63.2532,\n",
      "          -68.7495, -1500.9844,   254.9016,  -521.9299, -7430.6426,\n",
      "         -620.0928,   -50.7451], device='cuda:1')\n",
      "tensor([  1897.2810,    343.6959,    263.4781,   -157.2931,  -5665.4111,\n",
      "         -2061.3044,   2893.8311,  -1460.2933,   2397.7148,   -174.4832,\n",
      "         -4036.9500,  -3585.2034,  -2354.8596,  -2049.3774,   6710.3604,\n",
      "          1090.2439,   -151.2519,     38.0266,   4881.0781, -10316.6260,\n",
      "          3124.3960,  -1541.3387,   -786.6116,  -4093.2617,    -23.5916,\n",
      "          -118.1031,  -1678.9700,   1455.1888,   -741.5455,  -7460.7969,\n",
      "          -412.1021,    631.0304], device='cuda:1')\n",
      "tensor([ 2484.9438,   344.2017,   567.0350,   -46.3648, -6940.1992,\n",
      "        -2121.8923,  3090.4517, -1832.1949,   581.4255,  -161.6294,\n",
      "        -4449.9917, -5155.5762, -2959.1567, -1840.0981,  7167.5693,\n",
      "         -591.1539,   -43.7087,    37.0383,  7304.7969, -5626.5645,\n",
      "         2483.4453, -2133.3760,  -703.5955, -5276.0186,   -47.5876,\n",
      "         -149.8586, -1689.0699,  1906.2516, -1113.1907, -8471.7871,\n",
      "         -318.9142,    31.5428], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "check_pytorch_version()\n",
    "print(args)\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.makedirs(args.output_dir)\n",
    "msglogger = apputils.config_pylogger(os.path.join(script_dir, 'logging.conf'), args.name, args.output_dir)\n",
    "\n",
    "# Log various details about the execution environment.  It is sometimes useful\n",
    "# to refer to past experiment executions and this information may be useful.\n",
    "apputils.log_execution_env_state(sys.argv, gitroot=module_path)\n",
    "msglogger.debug(\"Distiller: %s\", distiller.__version__)\n",
    "\n",
    "start_epoch = 0\n",
    "best_epochs = [distiller.MutableNamedTuple({'epoch': 0, 'loss': float(\"inf\"), 'sparsity': 0})\n",
    "               for i in range(args.num_best_scores)]\n",
    "\n",
    "if args.deterministic:\n",
    "    # Experiment reproducibility is sometimes important.  Pete Warden expounded about this\n",
    "    # in his blog: https://petewarden.com/2018/03/19/the-machine-learning-reproducibility-crisis/\n",
    "    # In Pytorch, support for deterministic execution is still a bit clunky.\n",
    "    if args.workers > 1:\n",
    "        msglogger.error('ERROR: Setting --deterministic requires setting --workers/-j to 0 or 1')\n",
    "        exit(1)\n",
    "    # Use a well-known seed, for repeatability of experiments\n",
    "    torch.manual_seed(0)\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    cudnn.deterministic = True\n",
    "else:\n",
    "    # This issue: https://github.com/pytorch/pytorch/issues/3659\n",
    "    # Implies that cudnn.benchmark should respect cudnn.deterministic, but empirically we see that\n",
    "    # results are not re-produced when benchmark is set. So enabling only if deterministic mode disabled.\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "if args.gpus is not None:\n",
    "    try:\n",
    "        args.gpus = [int(s) for s in args.gpus.split(',')]\n",
    "    except ValueError:\n",
    "        msglogger.error('ERROR: Argument --gpus must be a comma-separated list of integers only')\n",
    "        exit(1)\n",
    "    available_gpus = torch.cuda.device_count()\n",
    "    for dev_id in args.gpus:\n",
    "        if dev_id >= available_gpus:\n",
    "            msglogger.error('ERROR: GPU device ID {0} requested, but only {1} devices available'\n",
    "                            .format(dev_id, available_gpus))\n",
    "            exit(1)\n",
    "    # Set default device in case the first one on the list != 0\n",
    "    torch.cuda.set_device(args.gpus[0])\n",
    "\n",
    "# # Infer the dataset from the model name\n",
    "# args.dataset = 'cifar10' if 'cifar' in args.arch else 'imagenet'\n",
    "# args.num_classes = 10 if args.dataset == 'cifar10' else 1000\n",
    "\n",
    "if args.earlyexit_thresholds:\n",
    "    args.num_exits = len(args.earlyexit_thresholds) + 1\n",
    "    args.loss_exits = [0] * args.num_exits\n",
    "    args.losses_exits = []\n",
    "    args.exiterrors = []\n",
    "\n",
    "# Create the model\n",
    "model = TransformerNet()\n",
    "model.cuda()\n",
    "vgg = Vgg16(requires_grad=False).cuda()\n",
    "style_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.mul(255))\n",
    "])\n",
    "style = utils.load_image(args.style_image, size=args.style_size)\n",
    "style = style_transform(style)\n",
    "style = style.repeat(args.batch_size, 1, 1, 1).cuda()\n",
    "\n",
    "features_style = vgg(utils.normalize_batch(style))\n",
    "gram_style = [utils.gram_matrix(y) for y in features_style]\n",
    "if args.pretrained:\n",
    "    if os.path.isfile(args.pretrained):\n",
    "        resumed_state_dict = torch.load(args.pretrained)\n",
    "        # remove saved deprecated running_* keys in InstanceNorm from the checkpoint\n",
    "        for k in list(resumed_state_dict.keys()):\n",
    "            if re.search(r'in\\d+\\.running_(mean|var)$', k):\n",
    "                del resumed_state_dict[k]\n",
    "        model.load_state_dict(resumed_state_dict)\n",
    "        msglogger.info('Loaded pretrained model from %s\\n', args.pretrained)\n",
    "\n",
    "compression_scheduler = None\n",
    "# Create a couple of logging backends.  TensorBoardLogger writes log files in a format\n",
    "# that can be read by Google's Tensor Board.  PythonLogger writes to the Python logger.\n",
    "tflogger = TensorBoardLogger(msglogger.logdir)\n",
    "pylogger = PythonLogger(msglogger)\n",
    "\n",
    "# capture thresholds for early-exit training\n",
    "if args.earlyexit_thresholds:\n",
    "    msglogger.info('=> using early-exit threshold values of %s', args.earlyexit_thresholds)\n",
    "\n",
    "# We can optionally resume from a checkpoint\n",
    "if args.resume:\n",
    "    model, compression_scheduler, start_epoch = apputils.load_checkpoint(\n",
    "        model, chkpt_file=args.resume)\n",
    "\n",
    "# Define loss function (criterion) and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), args.lr)\n",
    "msglogger.info('Optimizer Type: %s', type(optimizer))\n",
    "msglogger.info('Optimizer Args: %s', optimizer.defaults)\n",
    "\n",
    "# if args.ADC:\n",
    "#     return automated_deep_compression(model, criterion, pylogger, args)\n",
    "\n",
    "# This sample application can be invoked to produce various summary reports.\n",
    "# if args.summary:\n",
    "#     return summarize_model(model, args.train_dataset, which_summary=args.summary)\n",
    "\n",
    "# Load the datasets: the dataset to load is inferred from the model name passed\n",
    "# in args.arch.  The default dataset is ImageNet, but if args.arch contains the\n",
    "# substring \"_cifar\", then cifar10 is used.\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(args.image_size),\n",
    "    transforms.CenterCrop(args.image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.mul(255))\n",
    "])\n",
    "train_dataset = datasets.ImageFolder(args.train_dataset, transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size)\n",
    "val_dataset = datasets.ImageFolder(args.val_dataset, transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=args.batch_size)\n",
    "msglogger.info('Dataset sizes:\\n\\ttraining=%d\\n',\n",
    "               len(train_loader.sampler))\n",
    "\n",
    "activations_collectors = create_activation_stats_collectors(model, collection_phase=args.activation_stats)\n",
    "\n",
    "# if args.sensitivity is not None:\n",
    "#     return sensitivity_analysis(model, criterion, test_loader, pylogger, args)\n",
    "\n",
    "# if args.evaluate:\n",
    "#     return evaluate_model(model, criterion, test_loader, pylogger, activations_collectors, args)\n",
    "\n",
    "if args.compress:\n",
    "    # The main use-case for this sample application is CNN compression. Compression\n",
    "    # requires a compression schedule configuration file in YAML.\n",
    "    compression_scheduler = distiller.file_config(model, optimizer, args.compress)\n",
    "    # Model is re-transferred to GPU in case parameters were added (e.g. PACTQuantizer)\n",
    "    model.cuda()\n",
    "else:\n",
    "    compression_scheduler = distiller.CompressionScheduler(model)\n",
    "\n",
    "args.kd_policy = None\n",
    "if args.kd_teacher:\n",
    "    teacher = create_model(args.kd_pretrained, args.train_dataset, args.kd_teacher, device_ids=args.gpus)\n",
    "    if args.kd_resume:\n",
    "        teacher, _, _ = apputils.load_checkpoint(teacher, chkpt_file=args.kd_resume)\n",
    "    dlw = distiller.DistillationLossWeights(args.kd_distill_wt, args.kd_student_wt, args.kd_teacher_wt)\n",
    "    args.kd_policy = distiller.KnowledgeDistillationPolicy(model, teacher, args.kd_temp, dlw)\n",
    "    compression_scheduler.add_policy(args.kd_policy, starting_epoch=args.kd_start_epoch, ending_epoch=args.epochs,\n",
    "                                     frequency=1)\n",
    "\n",
    "    msglogger.info('\\nStudent-Teacher knowledge distillation enabled:')\n",
    "    msglogger.info('\\tTeacher Model: %s', args.kd_teacher)\n",
    "    msglogger.info('\\tTemperature: %s', args.kd_temp)\n",
    "    msglogger.info('\\tLoss Weights (distillation | student | teacher): %s',\n",
    "                   ' | '.join(['{:.2f}'.format(val) for val in dlw]))\n",
    "    msglogger.info('\\tStarting from Epoch: %s', args.kd_start_epoch)\n",
    "\n",
    "warmup(val_loader, model, criterion, optimizer, vgg, 0, compression_scheduler, [tflogger, pylogger],\n",
    "      args.print_freq, gram_style, args.content_weight, args.style_weight)\n",
    "\n",
    "if False:\n",
    "#for epoch in range(start_epoch, start_epoch + args.epochs):\n",
    "    # This is the main training loop.\n",
    "    msglogger.info('\\n')\n",
    "    if compression_scheduler:\n",
    "        compression_scheduler.on_epoch_begin(epoch)\n",
    "\n",
    "    # Train for one epoch\n",
    "    with collectors_context(activations_collectors[\"train\"]) as collectors:\n",
    "        train(train_loader, model, criterion, optimizer, vgg, epoch, compression_scheduler, [tflogger, pylogger],\n",
    "              args.print_freq, gram_style, args.content_weight, args.style_weight)\n",
    "        distiller.log_weights_sparsity(model, epoch, loggers=[tflogger, pylogger])\n",
    "        distiller.log_activation_statsitics(epoch, \"train\", loggers=[tflogger],\n",
    "                                            collector=collectors[\"sparsity\"])\n",
    "        if args.masks_sparsity:\n",
    "            msglogger.info(distiller.masks_sparsity_tbl_summary(model, compression_scheduler))\n",
    "\n",
    "    # evaluate on validation set\n",
    "    with collectors_context(activations_collectors[\"valid\"]) as collectors:\n",
    "        top1, top5, vloss = validate(val_loader, model, criterion, vgg, [pylogger], args, gram_style, epoch)\n",
    "        distiller.log_activation_statsitics(epoch, \"valid\", loggers=[tflogger],\n",
    "                                            collector=collectors[\"sparsity\"])\n",
    "        save_collectors_data(collectors, msglogger.logdir)\n",
    "\n",
    "    if compression_scheduler:\n",
    "        compression_scheduler.on_epoch_end(epoch, optimizer)\n",
    "\n",
    "    # remember best top1 and save checkpoint\n",
    "    #sparsity = distiller.model_sparsity(model)\n",
    "    is_best = vloss < best_epochs[0].loss\n",
    "    if is_best:\n",
    "        best_epochs[0].epoch = epoch\n",
    "        best_epochs[0].loss = vloss\n",
    "        best_epochs = sorted(best_epochs, key=lambda score: score.loss, reverse=True)\n",
    "    for score in reversed(best_epochs):\n",
    "        if score.loss > 0:\n",
    "            msglogger.info('==> Best Loss: %.3f on Epoch: %d', score.loss, score.epoch)\n",
    "    apputils.save_checkpoint(epoch, None, model, optimizer, compression_scheduler,\n",
    "                             best_epochs[0].loss, is_best, args.name, msglogger.logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STYLE_LOSS_KEY = 'Style Loss'\n",
    "CONTENT_LOSS_KEY = 'Content Loss'\n",
    "OVERALL_LOSS_KEY = 'Overall Loss'\n",
    "OBJECTIVE_LOSS_KEY = 'Objective Loss'\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, vgg, epoch, compression_scheduler, loggers,\n",
    "          print_freq, gram_style, content_weight, style_weight):\n",
    "    #     np.random.seed(args.seed)\n",
    "    #     torch.manual_seed(args.seed)\n",
    "    \"\"\"Training loop for one epoch.\"\"\"\n",
    "    losses = OrderedDict([(OVERALL_LOSS_KEY, tnt.AverageValueMeter()),\n",
    "                          (OBJECTIVE_LOSS_KEY, tnt.AverageValueMeter()),\n",
    "                          (STYLE_LOSS_KEY, tnt.AverageValueMeter()),\n",
    "                          (CONTENT_LOSS_KEY, tnt.AverageValueMeter())])\n",
    "\n",
    "    batch_time = tnt.AverageValueMeter()\n",
    "    total_samples = len(train_loader.sampler)\n",
    "    batch_size = train_loader.batch_size\n",
    "    steps_per_epoch = math.ceil(total_samples / batch_size)\n",
    "    msglogger.info('Training epoch: %d samples (%d per mini-batch)', total_samples, batch_size)\n",
    "\n",
    "    # Switch to train mode\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "\n",
    "    for train_step, (x, _) in enumerate(train_loader):\n",
    "        n_batch = len(x)\n",
    "\n",
    "        # Execute the forward phase, compute the output and measure loss\n",
    "        if compression_scheduler:\n",
    "            compression_scheduler.on_minibatch_begin(epoch, train_step, steps_per_epoch, optimizer)\n",
    "\n",
    "        x = x.to('cuda')\n",
    "        y = model(x)\n",
    "\n",
    "        y = utils.normalize_batch(y)\n",
    "        x = utils.normalize_batch(x)\n",
    "\n",
    "        features_y = vgg(y)\n",
    "        features_x = vgg(x)\n",
    "\n",
    "        content_loss = content_weight * criterion(features_y.relu2_2, features_x.relu2_2)\n",
    "\n",
    "        style_loss = 0.\n",
    "        for ft_y, gm_s in zip(features_y, gram_style):\n",
    "            gm_y = utils.gram_matrix(ft_y)\n",
    "            style_loss += criterion(gm_y, gm_s[:n_batch, :, :])\n",
    "        style_loss *= style_weight\n",
    "\n",
    "        loss = content_loss + style_loss\n",
    "\n",
    "        losses[STYLE_LOSS_KEY].add(style_loss.item())\n",
    "        losses[CONTENT_LOSS_KEY].add(content_loss.item())\n",
    "        losses[OBJECTIVE_LOSS_KEY].add(loss.item())\n",
    "\n",
    "        if compression_scheduler:\n",
    "            # Before running the backward phase, we allow the scheduler to modify the loss\n",
    "            # (e.g. add regularization loss)\n",
    "            agg_loss = compression_scheduler.before_backward_pass(epoch, train_step, steps_per_epoch, loss,\n",
    "                                                                  optimizer=optimizer, return_loss_components=True)\n",
    "            loss = agg_loss.overall_loss\n",
    "            losses[OVERALL_LOSS_KEY].add(loss.item())\n",
    "            for lc in agg_loss.loss_components:\n",
    "                if lc.name not in losses:\n",
    "                    losses[lc.name] = tnt.AverageValueMeter()\n",
    "                losses[lc.name].add(lc.value.item())\n",
    "\n",
    "        break\n",
    "        # Compute the gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if compression_scheduler:\n",
    "            compression_scheduler.on_minibatch_end(epoch, train_step, steps_per_epoch, optimizer)\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.add(time.time() - end)\n",
    "        steps_completed = (train_step+1)\n",
    "\n",
    "        if steps_completed % print_freq == 0:\n",
    "            stats_dict = OrderedDict()\n",
    "            for loss_name, meter in losses.items():\n",
    "                stats_dict[loss_name] = meter.mean\n",
    "            # stats_dict.update(errs)\n",
    "            stats_dict['LR'] = optimizer.param_groups[0]['lr']\n",
    "            stats_dict['Time'] = batch_time.mean\n",
    "            stats = ('Peformance/Training/', stats_dict)\n",
    "\n",
    "            # params = model.named_parameters() if args.log_params_histograms else None\n",
    "            params = None\n",
    "            distiller.log_training_progress(stats,\n",
    "                                            params,\n",
    "                                            epoch, steps_completed,\n",
    "                                            steps_per_epoch, print_freq,\n",
    "                                            loggers)\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion, vgg, loggers, args, gram_style, epoch=-1):\n",
    "    \"\"\"Model validation\"\"\"\n",
    "    if epoch > -1:\n",
    "        msglogger.info('--- validate (epoch=%d)-----------', epoch)\n",
    "    else:\n",
    "        msglogger.info('--- validate ---------------------')\n",
    "    return _validate(val_loader, model, criterion, vgg, loggers, args, gram_style, epoch)\n",
    "\n",
    "\n",
    "def _validate(data_loader, model, criterion, vgg, loggers, args, gram_style, epoch=-1):\n",
    "    \"\"\"Execute the validation/test loop.\"\"\"\n",
    "    losses = OrderedDict([(OBJECTIVE_LOSS_KEY, tnt.AverageValueMeter()),\n",
    "                          (STYLE_LOSS_KEY, tnt.AverageValueMeter()),\n",
    "                          (CONTENT_LOSS_KEY, tnt.AverageValueMeter())])\n",
    "    # classerr = tnt.ClassErrorMeter(accuracy=True, topk=(1, 5))\n",
    "\n",
    "    if args.earlyexit_thresholds:\n",
    "        # for Early Exit, we have a list of errors and losses for each of the exits.\n",
    "        args.exiterrors = []\n",
    "        args.losses_exits = []\n",
    "        for exitnum in range(args.num_exits):\n",
    "            # args.exiterrors.append(tnt.ClassErrorMeter(accuracy=True, topk=(1, 5)))\n",
    "            args.losses_exits.append(tnt.AverageValueMeter())\n",
    "        args.exit_taken = [0] * args.num_exits\n",
    "\n",
    "    batch_time = tnt.AverageValueMeter()\n",
    "    total_samples = len(data_loader.sampler)\n",
    "    batch_size = data_loader.batch_size\n",
    "    if args.display_confusion:\n",
    "        confusion = tnt.ConfusionMeter(args.num_classes)\n",
    "    total_steps = total_samples / batch_size\n",
    "    msglogger.info('%d samples (%d per mini-batch)', total_samples, batch_size)\n",
    "\n",
    "    steps_per_epoch = math.ceil(total_samples / batch_size)\n",
    "    msglogger.info('Validation epoch: %d samples (%d per mini-batch)', total_samples, batch_size)\n",
    "\n",
    "    # Switch to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for validation_step, (x, target) in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            if not args.earlyexit_thresholds:\n",
    "                n_batch = len(x)\n",
    "\n",
    "                x = x.to('cuda')\n",
    "                y = model(x)\n",
    "\n",
    "                y = utils.normalize_batch(y)\n",
    "                x = utils.normalize_batch(x)\n",
    "\n",
    "                features_y = vgg(y)\n",
    "                features_x = vgg(x)\n",
    "\n",
    "                content_loss = args.content_weight * criterion(features_y.relu2_2, features_x.relu2_2)\n",
    "\n",
    "                style_loss = 0.\n",
    "                for ft_y, gm_s in zip(features_y, gram_style):\n",
    "                    gm_y = utils.gram_matrix(ft_y)\n",
    "                    style_loss += criterion(gm_y, gm_s[:n_batch, :, :])\n",
    "                style_loss *= args.style_weight\n",
    "\n",
    "                loss = content_loss + style_loss\n",
    "\n",
    "                losses[STYLE_LOSS_KEY].add(style_loss.item())\n",
    "                losses[CONTENT_LOSS_KEY].add(content_loss.item())\n",
    "                losses[OBJECTIVE_LOSS_KEY].add(loss.item())\n",
    "                # classerr.add(output.data, target)\n",
    "                if args.display_confusion:\n",
    "                    confusion.add(output.data, target)\n",
    "            else:\n",
    "                earlyexit_validate_loss(output, target, criterion, args)\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.add(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            steps_completed = (validation_step+1)\n",
    "            if steps_completed % args.print_freq == 0:\n",
    "                if not args.earlyexit_thresholds:\n",
    "                    stats = ('',\n",
    "                            OrderedDict([('Objective Loss', losses[OBJECTIVE_LOSS_KEY].mean),\n",
    "                                         ('Style Loss', losses[STYLE_LOSS_KEY].mean),\n",
    "                                         ('Content Loss', losses[CONTENT_LOSS_KEY].mean)]))\n",
    "                else:\n",
    "                    stats_dict = OrderedDict()\n",
    "                    stats_dict['Test'] = validation_step\n",
    "                    for exitnum in range(args.num_exits):\n",
    "                        la_string = 'LossAvg' + str(exitnum)\n",
    "                        stats_dict[la_string] = args.losses_exits[exitnum].mean\n",
    "                        # Because of the nature of ClassErrorMeter, if an exit is never taken during the batch,\n",
    "                        # then accessing the value(k) will cause a divide by zero. So we'll build the OrderedDict\n",
    "                        # accordingly and we will not print for an exit error when that exit is never taken.\n",
    "                        if args.exit_taken[exitnum]:\n",
    "                            t1 = 'Top1_exit' + str(exitnum)\n",
    "                            t5 = 'Top5_exit' + str(exitnum)\n",
    "                            stats_dict[t1] = args.exiterrors[exitnum].value(1)\n",
    "                            stats_dict[t5] = args.exiterrors[exitnum].value(5)\n",
    "                    stats = ('Performance/Validation/', stats_dict)\n",
    "\n",
    "                distiller.log_training_progress(stats, None, epoch, steps_completed,\n",
    "                                                total_steps, args.print_freq, loggers)\n",
    "           \n",
    "\n",
    "    if not args.earlyexit_thresholds:\n",
    "        msglogger.info('==> Top1: %.3f    Top5: %.3f    Loss: %.3f\\n',\n",
    "                       0, 0, losses[OBJECTIVE_LOSS_KEY].mean)\n",
    "\n",
    "        if args.display_confusion:\n",
    "            msglogger.info('==> Confusion:\\n%s\\n', str(confusion.value()))\n",
    "        return 0, 0, losses[OBJECTIVE_LOSS_KEY].mean\n",
    "    else:\n",
    "        total_top1, total_top5, losses_exits_stats = earlyexit_validate_stats(args)\n",
    "        return total_top1, total_top5, losses_exits_stats[args.num_exits-1]\n",
    "    \n",
    "    \n",
    "\n",
    "def warmup(data_loader, model, criterion, optimizer, vgg, epoch, compression_scheduler, loggers, \n",
    "           print_freq, gram_style, content_weight, style_weight):\n",
    "    batch_time = tnt.AverageValueMeter()\n",
    "    total_samples = len(data_loader.sampler)\n",
    "    batch_size = data_loader.batch_size\n",
    "    steps_per_epoch = math.ceil(total_samples / batch_size)\n",
    "    msglogger.info('Warmup: %d samples (%d per mini-batch)', total_samples, batch_size)\n",
    "\n",
    "    # Switch to train mode\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "\n",
    "    # clean the gradients\n",
    "    # optimizer.zero_grad()\n",
    "\n",
    "    for train_step, (x, _) in enumerate(data_loader):\n",
    "        n_batch = len(x)\n",
    "\n",
    "        x = x.to('cuda')\n",
    "        y = model(x)\n",
    "\n",
    "        y = utils.normalize_batch(y)\n",
    "        x = utils.normalize_batch(x)\n",
    "\n",
    "        features_y = vgg(y)\n",
    "        features_x = vgg(x)\n",
    "\n",
    "        content_loss = content_weight * criterion(features_y.relu2_2, features_x.relu2_2)\n",
    "\n",
    "        style_loss = 0.\n",
    "        for ft_y, gm_s in zip(features_y, gram_style):\n",
    "            gm_y = utils.gram_matrix(ft_y)\n",
    "            style_loss += criterion(gm_y, gm_s[:n_batch, :, :])\n",
    "        style_loss *= style_weight\n",
    "\n",
    "        loss = content_loss + style_loss\n",
    "\n",
    "        # Compute the gradient and do SGD step\n",
    "        # optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # optimizer.step()\n",
    "        for name, layer in model.named_parameters():\n",
    "            if name == 'conv1.conv2d.weight':\n",
    "                print(layer[:,0,0,0])\n",
    "                print(layer.grad[:,0,0,0])\n",
    "        \n",
    "        if train_step == 4:\n",
    "            for name, layer in model.named_parameters():\n",
    "                layer.grad.div_(5)\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
