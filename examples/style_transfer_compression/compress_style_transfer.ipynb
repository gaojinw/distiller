{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Gradual Pruning Schedule\n",
    "\n",
    "Michael Zhu and Suyog Gupta, [\"To prune, or not to prune: exploring the efficacy of pruning for model compression\"](https://arxiv.org/pdf/1710.01878), 2017 NIPS Workshop on Machine Learning of Phones and other Consumer Devices<br>\n",
    "<br>\n",
    "After completing sensitivity analysis, decide on your pruning schedule.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Implementation of the gradual sparsity function](#Implementation-of-the-gradual-sparsity-function)\n",
    "2. [Visualize pruning schedule](#Visualize-pruning-schedule)\n",
    "3. [References](#References)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import traceback\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchnet.meter as tnt\n",
    "# script_dir = os.path.dirname(__file__)\n",
    "script_dir = os.path.abspath('/host/model_compression/distiller/examples/style_transfer_compression')\n",
    "module_path = os.path.abspath(os.path.join(script_dir, '..', '..'))\n",
    "try:\n",
    "    import distiller\n",
    "except ImportError:\n",
    "    sys.path.append(module_path)\n",
    "    import distiller\n",
    "import apputils\n",
    "from distiller.data_loggers import TensorBoardLogger, PythonLogger, ActivationSparsityCollector\n",
    "import distiller.quantization as quantization\n",
    "from models import ALL_MODEL_NAMES, create_model\n",
    "\n",
    "sys.path.append('/host/frameworks/examples/fast_neural_style/neural_style')\n",
    "from utils import *\n",
    "from neural_style import *\n",
    "\n",
    "# Logger handle\n",
    "msglogger = None\n",
    "\n",
    "\n",
    "def float_range(val_str):\n",
    "    val = float(val_str)\n",
    "    if val < 0 or val >= 1:\n",
    "        raise argparse.ArgumentTypeError('Must be >= 0 and < 1 (received {0})'.format(val_str))\n",
    "    return val\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Distiller image classification model compression')\n",
    "parser.add_argument('--dataset', default='/host/dataset/COCO', metavar='DIR', help='path to dataset')\n",
    "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('--epochs', default=90, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('-b', '--batch-size', default=256, type=int,\n",
    "                    metavar='N', help='mini-batch size (default: 256)')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,\n",
    "                    metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
    "                    help='momentum')\n",
    "parser.add_argument('--weight-decay', '--wd', default=1e-4, type=float,\n",
    "                    metavar='W', help='weight decay (default: 1e-4)')\n",
    "parser.add_argument('--print-freq', '-p', default=10, type=int,\n",
    "                    metavar='N', help='print frequency (default: 10)')\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',\n",
    "                    help='evaluate model on validation set')\n",
    "parser.add_argument('--pretrained', dest='pretrained', action='store_true',\n",
    "                    help='use pre-trained model')\n",
    "parser.add_argument('--act-stats', dest='activation_stats', action='store_true', default=False,\n",
    "                    help='collect activation statistics (WARNING: this slows down training)')\n",
    "parser.add_argument('--param-hist', dest='log_params_histograms', action='store_true', default=False,\n",
    "                    help='log the paramter tensors histograms to file (WARNING: this can use significant disk space)')\n",
    "SUMMARY_CHOICES = ['sparsity', 'compute', 'model', 'modules', 'png', 'png_w_params', 'onnx']\n",
    "parser.add_argument('--summary', type=str, choices=SUMMARY_CHOICES,\n",
    "                    help='print a summary of the model, and exit - options: ' +\n",
    "                    ' | '.join(SUMMARY_CHOICES))\n",
    "parser.add_argument('--compress', dest='compress', type=str, nargs='?', action='store',\n",
    "                    help='configuration file for pruning the model (default is to use hard-coded schedule)')\n",
    "parser.add_argument('--sense', dest='sensitivity', choices=['element', 'filter', 'channel'],\n",
    "                    help='test the sensitivity of layers to pruning')\n",
    "parser.add_argument('--extras', default=None, type=str,\n",
    "                    help='file with extra configuration information')\n",
    "parser.add_argument('--deterministic', '--det', action='store_true',\n",
    "                    help='Ensure deterministic execution for re-producible results.')\n",
    "parser.add_argument('--quantize', action='store_true',\n",
    "                    help='Apply 8-bit quantization to model before evaluation')\n",
    "parser.add_argument('--gpus', metavar='DEV_ID', default=None,\n",
    "                    help='Comma-separated list of GPU device IDs to be used (default is to use all available devices)')\n",
    "parser.add_argument('--name', '-n', metavar='NAME', default=None, help='Experiment name')\n",
    "parser.add_argument('--out-dir', '-o', dest='output_dir', default='logs', help='Path to dump logs and checkpoints')\n",
    "parser.add_argument('--validation-size', '--vs', type=float_range, default=0.1,\n",
    "                    help='Portion of training dataset to set aside for validation')\n",
    "parser.add_argument('--adc', dest='ADC', action='store_true', help='temp HACK')\n",
    "parser.add_argument('--adc-params', dest='ADC_params', default=None, help='temp HACK')\n",
    "parser.add_argument('--confusion', dest='display_confusion', default=False, action='store_true',\n",
    "                    help='Display the confusion matrix')\n",
    "parser.add_argument('--earlyexit_lossweights', type=float, nargs='*', dest='earlyexit_lossweights', default=None,\n",
    "                    help='List of loss weights for early exits (e.g. --lossweights 0.1 0.3)')\n",
    "parser.add_argument('--earlyexit_thresholds', type=float, nargs='*', dest='earlyexit_thresholds', default=None,\n",
    "                    help='List of EarlyExit thresholds (e.g. --earlyexit 1.2 0.9)')\n",
    "\n",
    "distiller.knowledge_distillation.add_distillation_args(parser, ALL_MODEL_NAMES, True)\n",
    "\n",
    "def check_pytorch_version():\n",
    "    if torch.__version__ < '0.4.0':\n",
    "        print(\"\\nNOTICE:\")\n",
    "        print(\"The Distiller \\'master\\' branch now requires at least PyTorch version 0.4.0 due to \"\n",
    "              \"PyTorch API changes which are not backward-compatible.\\n\"\n",
    "              \"Please install PyTorch 0.4.0 or its derivative.\\n\"\n",
    "              \"If you are using a virtual environment, do not forget to update it:\\n\"\n",
    "              \"  1. Deactivate the old environment\\n\"\n",
    "              \"  2. Install the new environment\\n\"\n",
    "              \"  3. Activate the new environment\")\n",
    "        exit(1)\n",
    "    else:\n",
    "        print(\"torch version: \" + torch.__version__)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# global msglogger\n",
    "# check_pytorch_version()\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/host/converter_trial/coreml/candy.pth\n"
     ]
    }
   ],
   "source": [
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "args = Object()\n",
    "args.dataset = '/host/dataset/COCO'\n",
    "args.epochs = 4\n",
    "args.batch_size = 2\n",
    "args.output_dir = './logs'\n",
    "args.name = None\n",
    "args.deterministic = False\n",
    "args.cuda = 1\n",
    "args.earlyexit_thresholds = None\n",
    "args.resume = '/host/converter_trial/coreml/candy.pth'\n",
    "args.lr = 1e-4\n",
    "args.style_size = None\n",
    "args.log_interval = 2\n",
    "args.log_params_histograms = False\n",
    "args.activation_stats = False\n",
    "args.image_size = 256\n",
    "args.pretrained = './pretrained/mosaic.pth'\n",
    "args.content_weight = 1e5\n",
    "args.style_weight = 1e10\n",
    "args.style_image = \"/host/frameworks/examples/fast_neural_style/images/style-images/candy.jpg\"\n",
    "args.compress = None # '../sensitivity-pruning/alexnet.schedule_sensitivity.yaml'\n",
    "print(args.resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log file for this run: /host/model_compression/distiller/examples/style_transfer_compression/logs/2018.11.08-090749/2018.11.08-090749.log\n",
      "Optimizer Type: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer Args: {'lr': 0.0001, 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'betas': (0.9, 0.999)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------\n",
      "Logging to TensorBoard - remember to execute the server:\n",
      "> tensorboard --logdir='./logs'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "\ttraining=82783\n",
      "\n",
      "\n",
      "\n",
      "Training epoch: 82783 samples (2 per mini-batch)\n",
      "Epoch: [0][    2/41392]    Overall Loss 3868320.500000    Objective Loss 3868320.500000    LR 0.000100    \n",
      "Epoch: [0][    4/41392]    Overall Loss 3743740.625000    Objective Loss 3743740.625000    LR 0.000100    \n",
      "Epoch: [0][    6/41392]    Overall Loss 3634853.958333    Objective Loss 3634853.958333    LR 0.000100    \n",
      "Epoch: [0][    8/41392]    Overall Loss 3531091.468750    Objective Loss 3531091.468750    LR 0.000100    \n",
      "Epoch: [0][   10/41392]    Overall Loss 3438809.625000    Objective Loss 3438809.625000    LR 0.000100    \n",
      "Epoch: [0][   12/41392]    Overall Loss 3343157.312500    Objective Loss 3343157.312500    LR 0.000100    \n",
      "Epoch: [0][   14/41392]    Overall Loss 3250093.464286    Objective Loss 3250093.464286    LR 0.000100    \n",
      "Epoch: [0][   16/41392]    Overall Loss 3179808.875000    Objective Loss 3179808.875000    LR 0.000100    \n",
      "Epoch: [0][   18/41392]    Overall Loss 3105655.291667    Objective Loss 3105655.291667    LR 0.000100    \n",
      "Epoch: [0][   20/41392]    Overall Loss 3034284.250000    Objective Loss 3034284.250000    LR 0.000100    \n",
      "Epoch: [0][   22/41392]    Overall Loss 2961439.704545    Objective Loss 2961439.704545    LR 0.000100    \n",
      "Epoch: [0][   24/41392]    Overall Loss 2893844.218750    Objective Loss 2893844.218750    LR 0.000100    \n",
      "Epoch: [0][   26/41392]    Overall Loss 2823944.668269    Objective Loss 2823944.668269    LR 0.000100    \n",
      "Epoch: [0][   28/41392]    Overall Loss 2756314.656250    Objective Loss 2756314.656250    LR 0.000100    \n",
      "Epoch: [0][   30/41392]    Overall Loss 2699027.620833    Objective Loss 2699027.620833    LR 0.000100    \n",
      "Epoch: [0][   32/41392]    Overall Loss 2644797.441406    Objective Loss 2644797.441406    LR 0.000100    \n",
      "Epoch: [0][   34/41392]    Overall Loss 2592391.047794    Objective Loss 2592391.047794    LR 0.000100    \n",
      "Epoch: [0][   36/41392]    Overall Loss 2537450.684028    Objective Loss 2537450.684028    LR 0.000100    \n",
      "Epoch: [0][   38/41392]    Overall Loss 2488581.302632    Objective Loss 2488581.302632    LR 0.000100    \n",
      "Epoch: [0][   40/41392]    Overall Loss 2442875.281250    Objective Loss 2442875.281250    LR 0.000100    \n",
      "Epoch: [0][   42/41392]    Overall Loss 2393947.613095    Objective Loss 2393947.613095    LR 0.000100    \n",
      "Epoch: [0][   44/41392]    Overall Loss 2349806.286932    Objective Loss 2349806.286932    LR 0.000100    \n",
      "Epoch: [0][   46/41392]    Overall Loss 2308708.355978    Objective Loss 2308708.355978    LR 0.000100    \n",
      "Epoch: [0][   48/41392]    Overall Loss 2266902.075521    Objective Loss 2266902.075521    LR 0.000100    \n",
      "Epoch: [0][   50/41392]    Overall Loss 2227552.087500    Objective Loss 2227552.087500    LR 0.000100    \n",
      "Epoch: [0][   52/41392]    Overall Loss 2188997.485577    Objective Loss 2188997.485577    LR 0.000100    \n",
      "Epoch: [0][   54/41392]    Overall Loss 2152145.703704    Objective Loss 2152145.703704    LR 0.000100    \n",
      "Epoch: [0][   56/41392]    Overall Loss 2115591.816964    Objective Loss 2115591.816964    LR 0.000100    \n",
      "Epoch: [0][   58/41392]    Overall Loss 2077132.993534    Objective Loss 2077132.993534    LR 0.000100    \n",
      "Epoch: [0][   60/41392]    Overall Loss 2047080.991667    Objective Loss 2047080.991667    LR 0.000100    \n",
      "Epoch: [0][   62/41392]    Overall Loss 2013426.473790    Objective Loss 2013426.473790    LR 0.000100    \n",
      "Epoch: [0][   64/41392]    Overall Loss 1981312.307617    Objective Loss 1981312.307617    LR 0.000100    \n",
      "Epoch: [0][   66/41392]    Overall Loss 1949992.573864    Objective Loss 1949992.573864    LR 0.000100    \n",
      "Epoch: [0][   68/41392]    Overall Loss 1921814.603860    Objective Loss 1921814.603860    LR 0.000100    \n",
      "Epoch: [0][   70/41392]    Overall Loss 1894718.572321    Objective Loss 1894718.572321    LR 0.000100    \n",
      "Epoch: [0][   72/41392]    Overall Loss 1867678.467882    Objective Loss 1867678.467882    LR 0.000100    \n",
      "Epoch: [0][   74/41392]    Overall Loss 1842564.479730    Objective Loss 1842564.479730    LR 0.000100    \n",
      "Epoch: [0][   76/41392]    Overall Loss 1818272.182566    Objective Loss 1818272.182566    LR 0.000100    \n",
      "Epoch: [0][   78/41392]    Overall Loss 1793096.773237    Objective Loss 1793096.773237    LR 0.000100    \n",
      "Epoch: [0][   80/41392]    Overall Loss 1770568.008594    Objective Loss 1770568.008594    LR 0.000100    \n",
      "Epoch: [0][   82/41392]    Overall Loss 1749302.954268    Objective Loss 1749302.954268    LR 0.000100    \n",
      "Epoch: [0][   84/41392]    Overall Loss 1726243.176339    Objective Loss 1726243.176339    LR 0.000100    \n",
      "Epoch: [0][   86/41392]    Overall Loss 1705993.007994    Objective Loss 1705993.007994    LR 0.000100    \n",
      "Epoch: [0][   88/41392]    Overall Loss 1685893.115057    Objective Loss 1685893.115057    LR 0.000100    \n",
      "Epoch: [0][   90/41392]    Overall Loss 1665317.755556    Objective Loss 1665317.755556    LR 0.000100    \n",
      "Epoch: [0][   92/41392]    Overall Loss 1646230.536685    Objective Loss 1646230.536685    LR 0.000100    \n",
      "Epoch: [0][   94/41392]    Overall Loss 1627712.482713    Objective Loss 1627712.482713    LR 0.000100    \n",
      "Epoch: [0][   96/41392]    Overall Loss 1609163.841797    Objective Loss 1609163.841797    LR 0.000100    \n",
      "Epoch: [0][   98/41392]    Overall Loss 1591475.598852    Objective Loss 1591475.598852    LR 0.000100    \n",
      "Epoch: [0][  100/41392]    Overall Loss 1574380.200625    Objective Loss 1574380.200625    LR 0.000100    \n",
      "Epoch: [0][  102/41392]    Overall Loss 1558705.903186    Objective Loss 1558705.903186    LR 0.000100    \n",
      "Epoch: [0][  104/41392]    Overall Loss 1543580.809495    Objective Loss 1543580.809495    LR 0.000100    \n",
      "Epoch: [0][  106/41392]    Overall Loss 1527903.225825    Objective Loss 1527903.225825    LR 0.000100    \n",
      "Epoch: [0][  108/41392]    Overall Loss 1513519.637731    Objective Loss 1513519.637731    LR 0.000100    \n",
      "Epoch: [0][  110/41392]    Overall Loss 1500071.498295    Objective Loss 1500071.498295    LR 0.000100    \n",
      "Epoch: [0][  112/41392]    Overall Loss 1485424.911830    Objective Loss 1485424.911830    LR 0.000100    \n",
      "Epoch: [0][  114/41392]    Overall Loss 1472327.598136    Objective Loss 1472327.598136    LR 0.000100    \n",
      "Epoch: [0][  116/41392]    Overall Loss 1460257.051185    Objective Loss 1460257.051185    LR 0.000100    \n",
      "Epoch: [0][  118/41392]    Overall Loss 1446751.650953    Objective Loss 1446751.650953    LR 0.000100    \n",
      "Epoch: [0][  120/41392]    Overall Loss 1434950.380729    Objective Loss 1434950.380729    LR 0.000100    \n",
      "Epoch: [0][  122/41392]    Overall Loss 1423212.637295    Objective Loss 1423212.637295    LR 0.000100    \n",
      "Epoch: [0][  124/41392]    Overall Loss 1411216.952117    Objective Loss 1411216.952117    LR 0.000100    \n",
      "Epoch: [0][  126/41392]    Overall Loss 1402029.039187    Objective Loss 1402029.039187    LR 0.000100    \n",
      "Epoch: [0][  128/41392]    Overall Loss 1390788.411133    Objective Loss 1390788.411133    LR 0.000100    \n",
      "Epoch: [0][  130/41392]    Overall Loss 1379382.639904    Objective Loss 1379382.639904    LR 0.000100    \n",
      "Epoch: [0][  132/41392]    Overall Loss 1368632.530303    Objective Loss 1368632.530303    LR 0.000100    \n",
      "Epoch: [0][  134/41392]    Overall Loss 1358796.803172    Objective Loss 1358796.803172    LR 0.000100    \n",
      "Epoch: [0][  136/41392]    Overall Loss 1349593.322610    Objective Loss 1349593.322610    LR 0.000100    \n",
      "Epoch: [0][  138/41392]    Overall Loss 1340175.087409    Objective Loss 1340175.087409    LR 0.000100    \n",
      "Epoch: [0][  140/41392]    Overall Loss 1329995.670089    Objective Loss 1329995.670089    LR 0.000100    \n",
      "Epoch: [0][  142/41392]    Overall Loss 1321911.243838    Objective Loss 1321911.243838    LR 0.000100    \n",
      "Epoch: [0][  144/41392]    Overall Loss 1312702.039497    Objective Loss 1312702.039497    LR 0.000100    \n",
      "Epoch: [0][  146/41392]    Overall Loss 1303884.077483    Objective Loss 1303884.077483    LR 0.000100    \n",
      "Epoch: [0][  148/41392]    Overall Loss 1296755.945946    Objective Loss 1296755.945946    LR 0.000100    \n",
      "Epoch: [0][  150/41392]    Overall Loss 1287875.357083    Objective Loss 1287875.357083    LR 0.000100    \n",
      "Epoch: [0][  152/41392]    Overall Loss 1280040.002878    Objective Loss 1280040.002878    LR 0.000100    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0][  154/41392]    Overall Loss 1271787.946023    Objective Loss 1271787.946023    LR 0.000100    \n",
      "Epoch: [0][  156/41392]    Overall Loss 1263964.281250    Objective Loss 1263964.281250    LR 0.000100    \n",
      "Epoch: [0][  158/41392]    Overall Loss 1257231.805380    Objective Loss 1257231.805380    LR 0.000100    \n",
      "Epoch: [0][  160/41392]    Overall Loss 1249811.459375    Objective Loss 1249811.459375    LR 0.000100    \n",
      "Epoch: [0][  162/41392]    Overall Loss 1242198.939815    Objective Loss 1242198.939815    LR 0.000100    \n",
      "Epoch: [0][  164/41392]    Overall Loss 1234983.033155    Objective Loss 1234983.033155    LR 0.000100    \n",
      "Epoch: [0][  166/41392]    Overall Loss 1227419.565136    Objective Loss 1227419.565136    LR 0.000100    \n",
      "Epoch: [0][  168/41392]    Overall Loss 1220812.448289    Objective Loss 1220812.448289    LR 0.000100    \n",
      "Epoch: [0][  170/41392]    Overall Loss 1214924.297794    Objective Loss 1214924.297794    LR 0.000100    \n",
      "Epoch: [0][  172/41392]    Overall Loss 1207796.138081    Objective Loss 1207796.138081    LR 0.000100    \n",
      "Epoch: [0][  174/41392]    Overall Loss 1201380.715158    Objective Loss 1201380.715158    LR 0.000100    \n",
      "Epoch: [0][  176/41392]    Overall Loss 1195411.180753    Objective Loss 1195411.180753    LR 0.000100    \n",
      "Epoch: [0][  178/41392]    Overall Loss 1189175.805126    Objective Loss 1189175.805126    LR 0.000100    \n",
      "Epoch: [0][  180/41392]    Overall Loss 1182786.047917    Objective Loss 1182786.047917    LR 0.000100    \n",
      "Epoch: [0][  182/41392]    Overall Loss 1176768.341346    Objective Loss 1176768.341346    LR 0.000100    \n",
      "Epoch: [0][  184/41392]    Overall Loss 1171502.113451    Objective Loss 1171502.113451    LR 0.000100    \n",
      "Epoch: [0][  186/41392]    Overall Loss 1166716.426411    Objective Loss 1166716.426411    LR 0.000100    \n",
      "Epoch: [0][  188/41392]    Overall Loss 1161643.366689    Objective Loss 1161643.366689    LR 0.000100    \n",
      "Epoch: [0][  190/41392]    Overall Loss 1155601.843421    Objective Loss 1155601.843421    LR 0.000100    \n",
      "Epoch: [0][  192/41392]    Overall Loss 1150340.027669    Objective Loss 1150340.027669    LR 0.000100    \n",
      "Epoch: [0][  194/41392]    Overall Loss 1144675.274485    Objective Loss 1144675.274485    LR 0.000100    \n",
      "Epoch: [0][  196/41392]    Overall Loss 1139632.898597    Objective Loss 1139632.898597    LR 0.000100    \n",
      "Epoch: [0][  198/41392]    Overall Loss 1134991.476957    Objective Loss 1134991.476957    LR 0.000100    \n",
      "Epoch: [0][  200/41392]    Overall Loss 1129475.075938    Objective Loss 1129475.075938    LR 0.000100    \n",
      "Epoch: [0][  202/41392]    Overall Loss 1124890.493812    Objective Loss 1124890.493812    LR 0.000100    \n",
      "Epoch: [0][  204/41392]    Overall Loss 1120170.993873    Objective Loss 1120170.993873    LR 0.000100    \n",
      "Epoch: [0][  206/41392]    Overall Loss 1115616.758799    Objective Loss 1115616.758799    LR 0.000100    \n",
      "Epoch: [0][  208/41392]    Overall Loss 1110433.476863    Objective Loss 1110433.476863    LR 0.000100    \n",
      "Epoch: [0][  210/41392]    Overall Loss 1106022.080655    Objective Loss 1106022.080655    LR 0.000100    \n",
      "Epoch: [0][  212/41392]    Overall Loss 1101763.664210    Objective Loss 1101763.664210    LR 0.000100    \n",
      "Epoch: [0][  214/41392]    Overall Loss 1098255.503797    Objective Loss 1098255.503797    LR 0.000100    \n",
      "Epoch: [0][  216/41392]    Overall Loss 1093752.162616    Objective Loss 1093752.162616    LR 0.000100    \n",
      "Epoch: [0][  218/41392]    Overall Loss 1089802.847190    Objective Loss 1089802.847190    LR 0.000100    \n",
      "Epoch: [0][  220/41392]    Overall Loss 1085517.064773    Objective Loss 1085517.064773    LR 0.000100    \n",
      "Epoch: [0][  222/41392]    Overall Loss 1081669.355574    Objective Loss 1081669.355574    LR 0.000100    \n",
      "Epoch: [0][  224/41392]    Overall Loss 1077460.226842    Objective Loss 1077460.226842    LR 0.000100    \n",
      "Epoch: [0][  226/41392]    Overall Loss 1073904.437777    Objective Loss 1073904.437777    LR 0.000100    \n",
      "Epoch: [0][  228/41392]    Overall Loss 1069822.254112    Objective Loss 1069822.254112    LR 0.000100    \n",
      "Epoch: [0][  230/41392]    Overall Loss 1066222.750815    Objective Loss 1066222.750815    LR 0.000100    \n",
      "Epoch: [0][  232/41392]    Overall Loss 1062614.349407    Objective Loss 1062614.349407    LR 0.000100    \n",
      "Epoch: [0][  234/41392]    Overall Loss 1059484.040064    Objective Loss 1059484.040064    LR 0.000100    \n",
      "Epoch: [0][  236/41392]    Overall Loss 1056090.104873    Objective Loss 1056090.104873    LR 0.000100    \n",
      "Epoch: [0][  238/41392]    Overall Loss 1052636.368172    Objective Loss 1052636.368172    LR 0.000100    \n",
      "Epoch: [0][  240/41392]    Overall Loss 1049179.669010    Objective Loss 1049179.669010    LR 0.000100    \n",
      "Epoch: [0][  242/41392]    Overall Loss 1045543.645919    Objective Loss 1045543.645919    LR 0.000100    \n",
      "Epoch: [0][  244/41392]    Overall Loss 1042651.245389    Objective Loss 1042651.245389    LR 0.000100    \n",
      "Epoch: [0][  246/41392]    Overall Loss 1038860.820122    Objective Loss 1038860.820122    LR 0.000100    \n",
      "Epoch: [0][  248/41392]    Overall Loss 1035034.784778    Objective Loss 1035034.784778    LR 0.000100    \n",
      "Epoch: [0][  250/41392]    Overall Loss 1032081.002000    Objective Loss 1032081.002000    LR 0.000100    \n",
      "Epoch: [0][  252/41392]    Overall Loss 1028863.232887    Objective Loss 1028863.232887    LR 0.000100    \n",
      "Epoch: [0][  254/41392]    Overall Loss 1026024.843996    Objective Loss 1026024.843996    LR 0.000100    \n",
      "Epoch: [0][  256/41392]    Overall Loss 1022489.064453    Objective Loss 1022489.064453    LR 0.000100    \n",
      "Epoch: [0][  258/41392]    Overall Loss 1019271.457607    Objective Loss 1019271.457607    LR 0.000100    \n",
      "Epoch: [0][  260/41392]    Overall Loss 1015761.020192    Objective Loss 1015761.020192    LR 0.000100    \n",
      "Epoch: [0][  262/41392]    Overall Loss 1012507.210401    Objective Loss 1012507.210401    LR 0.000100    \n",
      "Epoch: [0][  264/41392]    Overall Loss 1009493.550663    Objective Loss 1009493.550663    LR 0.000100    \n",
      "Epoch: [0][  266/41392]    Overall Loss 1006618.011278    Objective Loss 1006618.011278    LR 0.000100    \n",
      "Epoch: [0][  268/41392]    Overall Loss 1003218.065299    Objective Loss 1003218.065299    LR 0.000100    \n",
      "Epoch: [0][  270/41392]    Overall Loss 1000622.500926    Objective Loss 1000622.500926    LR 0.000100    \n",
      "Epoch: [0][  272/41392]    Overall Loss 997370.131434    Objective Loss 997370.131434    LR 0.000100    \n",
      "Epoch: [0][  274/41392]    Overall Loss 994437.260721    Objective Loss 994437.260721    LR 0.000100    \n",
      "Epoch: [0][  276/41392]    Overall Loss 991633.597600    Objective Loss 991633.597600    LR 0.000100    \n",
      "Epoch: [0][  278/41392]    Overall Loss 988745.638714    Objective Loss 988745.638714    LR 0.000100    \n",
      "Epoch: [0][  280/41392]    Overall Loss 985921.832366    Objective Loss 985921.832366    LR 0.000100    \n",
      "Epoch: [0][  282/41392]    Overall Loss 983278.907137    Objective Loss 983278.907137    LR 0.000100    \n",
      "Epoch: [0][  284/41392]    Overall Loss 980531.520467    Objective Loss 980531.520467    LR 0.000100    \n",
      "Epoch: [0][  286/41392]    Overall Loss 978411.389860    Objective Loss 978411.389860    LR 0.000100    \n",
      "Epoch: [0][  288/41392]    Overall Loss 975545.308377    Objective Loss 975545.308377    LR 0.000100    \n",
      "Epoch: [0][  290/41392]    Overall Loss 972937.798491    Objective Loss 972937.798491    LR 0.000100    \n",
      "Epoch: [0][  292/41392]    Overall Loss 970178.939426    Objective Loss 970178.939426    LR 0.000100    \n",
      "Epoch: [0][  294/41392]    Overall Loss 967679.588861    Objective Loss 967679.588861    LR 0.000100    \n",
      "Epoch: [0][  296/41392]    Overall Loss 965377.144215    Objective Loss 965377.144215    LR 0.000100    \n",
      "Epoch: [0][  298/41392]    Overall Loss 963634.572148    Objective Loss 963634.572148    LR 0.000100    \n",
      "Epoch: [0][  300/41392]    Overall Loss 961080.628333    Objective Loss 961080.628333    LR 0.000100    \n",
      "Epoch: [0][  302/41392]    Overall Loss 958526.067881    Objective Loss 958526.067881    LR 0.000100    \n",
      "Epoch: [0][  304/41392]    Overall Loss 955907.911390    Objective Loss 955907.911390    LR 0.000100    \n",
      "Epoch: [0][  306/41392]    Overall Loss 953551.977124    Objective Loss 953551.977124    LR 0.000100    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0][  308/41392]    Overall Loss 950886.046469    Objective Loss 950886.046469    LR 0.000100    \n",
      "Epoch: [0][  310/41392]    Overall Loss 948448.489516    Objective Loss 948448.489516    LR 0.000100    \n",
      "Epoch: [0][  312/41392]    Overall Loss 946436.228165    Objective Loss 946436.228165    LR 0.000100    \n",
      "Epoch: [0][  314/41392]    Overall Loss 944041.170183    Objective Loss 944041.170183    LR 0.000100    \n",
      "Epoch: [0][  316/41392]    Overall Loss 942709.234177    Objective Loss 942709.234177    LR 0.000100    \n",
      "Epoch: [0][  318/41392]    Overall Loss 940364.474057    Objective Loss 940364.474057    LR 0.000100    \n",
      "Epoch: [0][  320/41392]    Overall Loss 938224.546094    Objective Loss 938224.546094    LR 0.000100    \n",
      "Epoch: [0][  322/41392]    Overall Loss 936342.519410    Objective Loss 936342.519410    LR 0.000100    \n",
      "Epoch: [0][  324/41392]    Overall Loss 933841.230517    Objective Loss 933841.230517    LR 0.000100    \n",
      "Epoch: [0][  326/41392]    Overall Loss 931981.277224    Objective Loss 931981.277224    LR 0.000100    \n",
      "Epoch: [0][  328/41392]    Overall Loss 930044.924352    Objective Loss 930044.924352    LR 0.000100    \n",
      "Epoch: [0][  330/41392]    Overall Loss 927991.603409    Objective Loss 927991.603409    LR 0.000100    \n",
      "Epoch: [0][  332/41392]    Overall Loss 925938.656438    Objective Loss 925938.656438    LR 0.000100    \n",
      "Epoch: [0][  334/41392]    Overall Loss 924094.451722    Objective Loss 924094.451722    LR 0.000100    \n",
      "Epoch: [0][  336/41392]    Overall Loss 922100.170201    Objective Loss 922100.170201    LR 0.000100    \n",
      "Epoch: [0][  338/41392]    Overall Loss 920228.316383    Objective Loss 920228.316383    LR 0.000100    \n",
      "Epoch: [0][  340/41392]    Overall Loss 918143.741728    Objective Loss 918143.741728    LR 0.000100    \n",
      "Epoch: [0][  342/41392]    Overall Loss 916203.880300    Objective Loss 916203.880300    LR 0.000100    \n",
      "Epoch: [0][  344/41392]    Overall Loss 914514.345022    Objective Loss 914514.345022    LR 0.000100    \n",
      "Epoch: [0][  346/41392]    Overall Loss 912808.409140    Objective Loss 912808.409140    LR 0.000100    \n",
      "Epoch: [0][  348/41392]    Overall Loss 911161.001437    Objective Loss 911161.001437    LR 0.000100    \n",
      "Epoch: [0][  350/41392]    Overall Loss 909180.875357    Objective Loss 909180.875357    LR 0.000100    \n",
      "Epoch: [0][  352/41392]    Overall Loss 907061.909979    Objective Loss 907061.909979    LR 0.000100    \n",
      "Epoch: [0][  354/41392]    Overall Loss 905131.072387    Objective Loss 905131.072387    LR 0.000100    \n",
      "Epoch: [0][  356/41392]    Overall Loss 903358.352353    Objective Loss 903358.352353    LR 0.000100    \n",
      "Epoch: [0][  358/41392]    Overall Loss 901755.613827    Objective Loss 901755.613827    LR 0.000100    \n",
      "Epoch: [0][  360/41392]    Overall Loss 900118.468750    Objective Loss 900118.468750    LR 0.000100    \n",
      "Epoch: [0][  362/41392]    Overall Loss 898397.454593    Objective Loss 898397.454593    LR 0.000100    \n",
      "Epoch: [0][  364/41392]    Overall Loss 896992.304430    Objective Loss 896992.304430    LR 0.000100    \n",
      "Epoch: [0][  366/41392]    Overall Loss 895346.084699    Objective Loss 895346.084699    LR 0.000100    \n",
      "Epoch: [0][  368/41392]    Overall Loss 893932.736923    Objective Loss 893932.736923    LR 0.000100    \n",
      "Epoch: [0][  370/41392]    Overall Loss 892249.772804    Objective Loss 892249.772804    LR 0.000100    \n",
      "Epoch: [0][  372/41392]    Overall Loss 890834.603663    Objective Loss 890834.603663    LR 0.000100    \n",
      "Epoch: [0][  374/41392]    Overall Loss 889201.629846    Objective Loss 889201.629846    LR 0.000100    \n",
      "Epoch: [0][  376/41392]    Overall Loss 887448.147773    Objective Loss 887448.147773    LR 0.000100    \n",
      "Epoch: [0][  378/41392]    Overall Loss 885564.782407    Objective Loss 885564.782407    LR 0.000100    \n",
      "Epoch: [0][  380/41392]    Overall Loss 884084.749013    Objective Loss 884084.749013    LR 0.000100    \n",
      "Epoch: [0][  382/41392]    Overall Loss 882729.609948    Objective Loss 882729.609948    LR 0.000100    \n",
      "Epoch: [0][  384/41392]    Overall Loss 881434.379557    Objective Loss 881434.379557    LR 0.000100    \n",
      "Epoch: [0][  386/41392]    Overall Loss 879894.227655    Objective Loss 879894.227655    LR 0.000100    \n",
      "Epoch: [0][  388/41392]    Overall Loss 878520.829575    Objective Loss 878520.829575    LR 0.000100    \n",
      "Epoch: [0][  390/41392]    Overall Loss 877207.702083    Objective Loss 877207.702083    LR 0.000100    \n",
      "Epoch: [0][  392/41392]    Overall Loss 875597.806920    Objective Loss 875597.806920    LR 0.000100    \n",
      "Epoch: [0][  394/41392]    Overall Loss 874306.973350    Objective Loss 874306.973350    LR 0.000100    \n",
      "Epoch: [0][  396/41392]    Overall Loss 872745.724432    Objective Loss 872745.724432    LR 0.000100    \n",
      "Epoch: [0][  398/41392]    Overall Loss 871400.568781    Objective Loss 871400.568781    LR 0.000100    \n",
      "Epoch: [0][  400/41392]    Overall Loss 870485.491250    Objective Loss 870485.491250    LR 0.000100    \n",
      "Epoch: [0][  402/41392]    Overall Loss 869099.193719    Objective Loss 869099.193719    LR 0.000100    \n",
      "Epoch: [0][  404/41392]    Overall Loss 867465.010056    Objective Loss 867465.010056    LR 0.000100    \n",
      "Epoch: [0][  406/41392]    Overall Loss 865961.747691    Objective Loss 865961.747691    LR 0.000100    \n",
      "Epoch: [0][  408/41392]    Overall Loss 864921.761183    Objective Loss 864921.761183    LR 0.000100    \n",
      "Epoch: [0][  410/41392]    Overall Loss 863843.328506    Objective Loss 863843.328506    LR 0.000100    \n",
      "Epoch: [0][  412/41392]    Overall Loss 862392.370146    Objective Loss 862392.370146    LR 0.000100    \n",
      "Epoch: [0][  414/41392]    Overall Loss 861077.271135    Objective Loss 861077.271135    LR 0.000100    \n",
      "Epoch: [0][  416/41392]    Overall Loss 859772.251202    Objective Loss 859772.251202    LR 0.000100    \n",
      "Epoch: [0][  418/41392]    Overall Loss 858563.133672    Objective Loss 858563.133672    LR 0.000100    \n",
      "Epoch: [0][  420/41392]    Overall Loss 857469.051190    Objective Loss 857469.051190    LR 0.000100    \n",
      "Epoch: [0][  422/41392]    Overall Loss 856348.755332    Objective Loss 856348.755332    LR 0.000100    \n",
      "Epoch: [0][  424/41392]    Overall Loss 855162.741156    Objective Loss 855162.741156    LR 0.000100    \n",
      "Epoch: [0][  426/41392]    Overall Loss 854201.066315    Objective Loss 854201.066315    LR 0.000100    \n",
      "Epoch: [0][  428/41392]    Overall Loss 852859.239778    Objective Loss 852859.239778    LR 0.000100    \n",
      "Epoch: [0][  430/41392]    Overall Loss 851455.454651    Objective Loss 851455.454651    LR 0.000100    \n",
      "Epoch: [0][  432/41392]    Overall Loss 850421.686632    Objective Loss 850421.686632    LR 0.000100    \n",
      "Epoch: [0][  434/41392]    Overall Loss 849333.437356    Objective Loss 849333.437356    LR 0.000100    \n",
      "Epoch: [0][  436/41392]    Overall Loss 848378.732081    Objective Loss 848378.732081    LR 0.000100    \n",
      "Epoch: [0][  438/41392]    Overall Loss 846951.154466    Objective Loss 846951.154466    LR 0.000100    \n",
      "Epoch: [0][  440/41392]    Overall Loss 845762.331037    Objective Loss 845762.331037    LR 0.000100    \n",
      "Epoch: [0][  442/41392]    Overall Loss 844603.051541    Objective Loss 844603.051541    LR 0.000100    \n",
      "Epoch: [0][  444/41392]    Overall Loss 843118.196720    Objective Loss 843118.196720    LR 0.000100    \n",
      "Epoch: [0][  446/41392]    Overall Loss 842046.314112    Objective Loss 842046.314112    LR 0.000100    \n",
      "Epoch: [0][  448/41392]    Overall Loss 840697.242815    Objective Loss 840697.242815    LR 0.000100    \n",
      "Epoch: [0][  450/41392]    Overall Loss 839982.886319    Objective Loss 839982.886319    LR 0.000100    \n",
      "Epoch: [0][  452/41392]    Overall Loss 838769.569068    Objective Loss 838769.569068    LR 0.000100    \n",
      "Epoch: [0][  454/41392]    Overall Loss 837559.070416    Objective Loss 837559.070416    LR 0.000100    \n",
      "Epoch: [0][  456/41392]    Overall Loss 836333.440447    Objective Loss 836333.440447    LR 0.000100    \n",
      "Epoch: [0][  458/41392]    Overall Loss 835365.604189    Objective Loss 835365.604189    LR 0.000100    \n",
      "Epoch: [0][  460/41392]    Overall Loss 834279.292731    Objective Loss 834279.292731    LR 0.000100    \n",
      "Epoch: [0][  462/41392]    Overall Loss 833052.992357    Objective Loss 833052.992357    LR 0.000100    \n",
      "Epoch: [0][  464/41392]    Overall Loss 831813.960533    Objective Loss 831813.960533    LR 0.000100    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0][  466/41392]    Overall Loss 830619.695815    Objective Loss 830619.695815    LR 0.000100    \n",
      "Epoch: [0][  468/41392]    Overall Loss 829419.234375    Objective Loss 829419.234375    LR 0.000100    \n",
      "Epoch: [0][  470/41392]    Overall Loss 828592.099202    Objective Loss 828592.099202    LR 0.000100    \n",
      "Epoch: [0][  472/41392]    Overall Loss 827308.096398    Objective Loss 827308.096398    LR 0.000100    \n",
      "Epoch: [0][  474/41392]    Overall Loss 826174.430775    Objective Loss 826174.430775    LR 0.000100    \n",
      "Epoch: [0][  476/41392]    Overall Loss 825169.913603    Objective Loss 825169.913603    LR 0.000100    \n",
      "Epoch: [0][  478/41392]    Overall Loss 824192.156773    Objective Loss 824192.156773    LR 0.000100    \n",
      "Epoch: [0][  480/41392]    Overall Loss 823381.705599    Objective Loss 823381.705599    LR 0.000100    \n",
      "Epoch: [0][  482/41392]    Overall Loss 822438.968361    Objective Loss 822438.968361    LR 0.000100    \n",
      "Epoch: [0][  484/41392]    Overall Loss 821278.273760    Objective Loss 821278.273760    LR 0.000100    \n",
      "Epoch: [0][  486/41392]    Overall Loss 820033.055941    Objective Loss 820033.055941    LR 0.000100    \n",
      "Epoch: [0][  488/41392]    Overall Loss 818936.158299    Objective Loss 818936.158299    LR 0.000100    \n",
      "Epoch: [0][  490/41392]    Overall Loss 817983.414158    Objective Loss 817983.414158    LR 0.000100    \n",
      "Epoch: [0][  492/41392]    Overall Loss 816848.774517    Objective Loss 816848.774517    LR 0.000100    \n",
      "Epoch: [0][  494/41392]    Overall Loss 815729.799532    Objective Loss 815729.799532    LR 0.000100    \n",
      "Epoch: [0][  496/41392]    Overall Loss 814683.127205    Objective Loss 814683.127205    LR 0.000100    \n",
      "Epoch: [0][  498/41392]    Overall Loss 814067.417608    Objective Loss 814067.417608    LR 0.000100    \n",
      "Epoch: [0][  500/41392]    Overall Loss 813045.247688    Objective Loss 813045.247688    LR 0.000100    \n",
      "Epoch: [0][  502/41392]    Overall Loss 812145.794136    Objective Loss 812145.794136    LR 0.000100    \n",
      "Epoch: [0][  504/41392]    Overall Loss 811161.063306    Objective Loss 811161.063306    LR 0.000100    \n",
      "Epoch: [0][  506/41392]    Overall Loss 810142.021677    Objective Loss 810142.021677    LR 0.000100    \n",
      "Epoch: [0][  508/41392]    Overall Loss 809042.044353    Objective Loss 809042.044353    LR 0.000100    \n",
      "Epoch: [0][  510/41392]    Overall Loss 807916.126348    Objective Loss 807916.126348    LR 0.000100    \n",
      "Epoch: [0][  512/41392]    Overall Loss 806957.180664    Objective Loss 806957.180664    LR 0.000100    \n",
      "Epoch: [0][  514/41392]    Overall Loss 806106.874635    Objective Loss 806106.874635    LR 0.000100    \n",
      "Epoch: [0][  516/41392]    Overall Loss 805464.490068    Objective Loss 805464.490068    LR 0.000100    \n",
      "Epoch: [0][  518/41392]    Overall Loss 804680.741071    Objective Loss 804680.741071    LR 0.000100    \n",
      "Epoch: [0][  520/41392]    Overall Loss 803850.771995    Objective Loss 803850.771995    LR 0.000100    \n",
      "Epoch: [0][  522/41392]    Overall Loss 802891.503891    Objective Loss 802891.503891    LR 0.000100    \n",
      "Epoch: [0][  524/41392]    Overall Loss 801978.323771    Objective Loss 801978.323771    LR 0.000100    \n",
      "Epoch: [0][  526/41392]    Overall Loss 801295.724750    Objective Loss 801295.724750    LR 0.000100    \n",
      "Epoch: [0][  528/41392]    Overall Loss 800465.690578    Objective Loss 800465.690578    LR 0.000100    \n",
      "Epoch: [0][  530/41392]    Overall Loss 799644.287736    Objective Loss 799644.287736    LR 0.000100    \n",
      "Epoch: [0][  532/41392]    Overall Loss 798879.161537    Objective Loss 798879.161537    LR 0.000100    \n",
      "Epoch: [0][  534/41392]    Overall Loss 797898.114993    Objective Loss 797898.114993    LR 0.000100    \n",
      "Epoch: [0][  536/41392]    Overall Loss 796838.567922    Objective Loss 796838.567922    LR 0.000100    \n",
      "Epoch: [0][  538/41392]    Overall Loss 796031.223571    Objective Loss 796031.223571    LR 0.000100    \n",
      "Epoch: [0][  540/41392]    Overall Loss 795357.784086    Objective Loss 795357.784086    LR 0.000100    \n",
      "Epoch: [0][  542/41392]    Overall Loss 794465.141432    Objective Loss 794465.141432    LR 0.000100    \n",
      "Epoch: [0][  544/41392]    Overall Loss 793664.634938    Objective Loss 793664.634938    LR 0.000100    \n",
      "Epoch: [0][  546/41392]    Overall Loss 792849.881010    Objective Loss 792849.881010    LR 0.000100    \n",
      "Epoch: [0][  548/41392]    Overall Loss 791976.514542    Objective Loss 791976.514542    LR 0.000100    \n",
      "Epoch: [0][  550/41392]    Overall Loss 791117.517784    Objective Loss 791117.517784    LR 0.000100    \n",
      "Epoch: [0][  552/41392]    Overall Loss 790352.359318    Objective Loss 790352.359318    LR 0.000100    \n",
      "Epoch: [0][  554/41392]    Overall Loss 789482.620995    Objective Loss 789482.620995    LR 0.000100    \n",
      "Epoch: [0][  556/41392]    Overall Loss 788489.727349    Objective Loss 788489.727349    LR 0.000100    \n",
      "Epoch: [0][  558/41392]    Overall Loss 787665.131664    Objective Loss 787665.131664    LR 0.000100    \n",
      "Epoch: [0][  560/41392]    Overall Loss 786826.348717    Objective Loss 786826.348717    LR 0.000100    \n",
      "Epoch: [0][  562/41392]    Overall Loss 786060.851257    Objective Loss 786060.851257    LR 0.000100    \n",
      "Epoch: [0][  564/41392]    Overall Loss 785520.339262    Objective Loss 785520.339262    LR 0.000100    \n",
      "Epoch: [0][  566/41392]    Overall Loss 784706.383779    Objective Loss 784706.383779    LR 0.000100    \n",
      "Epoch: [0][  568/41392]    Overall Loss 784081.130667    Objective Loss 784081.130667    LR 0.000100    \n",
      "Epoch: [0][  570/41392]    Overall Loss 783682.078235    Objective Loss 783682.078235    LR 0.000100    \n",
      "Epoch: [0][  572/41392]    Overall Loss 782861.278136    Objective Loss 782861.278136    LR 0.000100    \n",
      "Epoch: [0][  574/41392]    Overall Loss 782618.674597    Objective Loss 782618.674597    LR 0.000100    \n",
      "Epoch: [0][  576/41392]    Overall Loss 781990.230740    Objective Loss 781990.230740    LR 0.000100    \n",
      "Epoch: [0][  578/41392]    Overall Loss 781164.373648    Objective Loss 781164.373648    LR 0.000100    \n",
      "Epoch: [0][  580/41392]    Overall Loss 780340.042241    Objective Loss 780340.042241    LR 0.000100    \n",
      "Epoch: [0][  582/41392]    Overall Loss 779698.805198    Objective Loss 779698.805198    LR 0.000100    \n",
      "Epoch: [0][  584/41392]    Overall Loss 778824.452911    Objective Loss 778824.452911    LR 0.000100    \n",
      "Epoch: [0][  586/41392]    Overall Loss 778137.964057    Objective Loss 778137.964057    LR 0.000100    \n",
      "Epoch: [0][  588/41392]    Overall Loss 777330.721620    Objective Loss 777330.721620    LR 0.000100    \n",
      "Epoch: [0][  590/41392]    Overall Loss 776600.644333    Objective Loss 776600.644333    LR 0.000100    \n",
      "Epoch: [0][  592/41392]    Overall Loss 776221.873733    Objective Loss 776221.873733    LR 0.000100    \n",
      "Epoch: [0][  594/41392]    Overall Loss 775501.079440    Objective Loss 775501.079440    LR 0.000100    \n",
      "Epoch: [0][  596/41392]    Overall Loss 774763.389576    Objective Loss 774763.389576    LR 0.000100    \n",
      "Epoch: [0][  598/41392]    Overall Loss 774016.396530    Objective Loss 774016.396530    LR 0.000100    \n",
      "Epoch: [0][  600/41392]    Overall Loss 773344.229167    Objective Loss 773344.229167    LR 0.000100    \n",
      "Epoch: [0][  602/41392]    Overall Loss 772551.212209    Objective Loss 772551.212209    LR 0.000100    \n",
      "Epoch: [0][  604/41392]    Overall Loss 771855.698469    Objective Loss 771855.698469    LR 0.000100    \n",
      "Epoch: [0][  606/41392]    Overall Loss 771051.156353    Objective Loss 771051.156353    LR 0.000100    \n",
      "Epoch: [0][  608/41392]    Overall Loss 770386.646587    Objective Loss 770386.646587    LR 0.000100    \n",
      "Epoch: [0][  610/41392]    Overall Loss 769825.073975    Objective Loss 769825.073975    LR 0.000100    \n",
      "Epoch: [0][  612/41392]    Overall Loss 769186.599980    Objective Loss 769186.599980    LR 0.000100    \n",
      "Epoch: [0][  614/41392]    Overall Loss 768479.207553    Objective Loss 768479.207553    LR 0.000100    \n",
      "Epoch: [0][  616/41392]    Overall Loss 767816.684963    Objective Loss 767816.684963    LR 0.000100    \n",
      "Epoch: [0][  618/41392]    Overall Loss 767084.559871    Objective Loss 767084.559871    LR 0.000100    \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e5ba9b4807a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# Train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         train(train_loader, model, optimizer, vgg, epoch, compression_scheduler, [tflogger, pylogger],\n\u001b[0;32m--> 132\u001b[0;31m               args.log_interval, args.style_image, args.style_size, args.content_weight, args.style_weight)\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mdistiller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_weights_sparsity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloggers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtflogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpylogger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_stats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-5021dc7f87a2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, optimizer, vgg, epoch, compression_scheduler, loggers, log_interval, style_image, style_size, content_weight, style_weight)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mn_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mn_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \"\"\"\n\u001b[1;32m    100\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \"\"\"\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not os.path.exists(args.output_dir):\n",
    "    os.makedirs(args.output_dir)\n",
    "msglogger = apputils.config_pylogger(os.path.join(script_dir, 'logging.conf'), args.name, args.output_dir)\n",
    "\n",
    "# Log various details about the execution environment.  It is sometimes useful\n",
    "# to refer to past experiment executions and this information may be useful.\n",
    "apputils.log_execution_env_state(sys.argv, gitroot=module_path)\n",
    "msglogger.debug(\"Distiller: %s\", distiller.__version__)\n",
    "\n",
    "start_epoch = 0\n",
    "best_top1 = 0\n",
    "best_epoch = 0\n",
    "\n",
    "if args.deterministic:\n",
    "    # Experiment reproducibility is sometimes important.  Pete Warden expounded about this\n",
    "    # in his blog: https://petewarden.com/2018/03/19/the-machine-learning-reproducibility-crisis/\n",
    "    # In Pytorch, support for deterministic execution is still a bit clunky.\n",
    "    if args.workers > 1:\n",
    "        msglogger.error('ERROR: Setting --deterministic requires setting --workers/-j to 0 or 1')\n",
    "        exit(1)\n",
    "        # Use a well-known seed, for repeatability of experiments\n",
    "        torch.manual_seed(0)\n",
    "        random.seed(0)\n",
    "        np.random.seed(0)\n",
    "        cudnn.deterministic = True\n",
    "    else:\n",
    "        # This issue: https://github.com/pytorch/pytorch/issues/3659\n",
    "        # Implies that cudnn.benchmark should respect cudnn.deterministic, but empirically we see that\n",
    "        # results are not re-produced when benchmark is set. So enabling only if deterministic mode disabled.\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "# if args.gpus is not None:\n",
    "#     try:\n",
    "#         args.gpus = [int(s) for s in args.gpus.split(',')]\n",
    "#     except ValueError:\n",
    "#         msglogger.error('ERROR: Argument --gpus must be a comma-separated list of integers only')\n",
    "#         exit(1)\n",
    "#     available_gpus = torch.cuda.device_count()\n",
    "#     for dev_id in args.gpus:\n",
    "#         if dev_id >= available_gpus:\n",
    "#             msglogger.error('ERROR: GPU device ID {0} requested, but only {1} devices available'\n",
    "#                             .format(dev_id, available_gpus))\n",
    "#             exit(1)\n",
    "#     # Set default device in case the first one on the list != 0\n",
    "#     torch.cuda.set_device(args.gpus[0])\n",
    "    \n",
    "# # Infer the dataset from the model name\n",
    "# args.dataset = 'cifar10' if 'cifar' in args.arch else 'imagenet'\n",
    "# args.num_classes = 10 if args.dataset == 'cifar10' else 1000\n",
    "\n",
    "if args.earlyexit_thresholds:\n",
    "    args.num_exits = len(args.earlyexit_thresholds) + 1\n",
    "    args.loss_exits = [0] * args.num_exits\n",
    "    args.losses_exits = []\n",
    "    args.exiterrors = []\n",
    "    \n",
    "# Create the model\n",
    "model = TransformerNet()\n",
    "if args.cuda:\n",
    "    device = torch.device(\"cuda:{}\".format(args.cuda-1))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "vgg = Vgg16(requires_grad=False).to(device)\n",
    "style_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.mul(255))\n",
    "])\n",
    "\n",
    "compression_scheduler = None\n",
    "# Create a couple of logging backends.  TensorBoardLogger writes log files in a format\n",
    "# that can be read by Google's Tensor Board.  PythonLogger writes to the Python logger.\n",
    "tflogger = TensorBoardLogger(msglogger.logdir)\n",
    "pylogger = PythonLogger(msglogger)\n",
    "\n",
    "# capture thresholds for early-exit training\n",
    "if args.earlyexit_thresholds:\n",
    "    msglogger.info('=> using early-exit threshold values of %s', args.earlyexit_thresholds)\n",
    "\n",
    "# We can optionally resume from a checkpoint\n",
    "if args.resume:\n",
    "    resumed_state_dict = torch.load(args.resume)\n",
    "    # remove saved deprecated running_* keys in InstanceNorm from the checkpoint\n",
    "    for k in list(resumed_state_dict.keys()):\n",
    "        if re.search(r'in\\d+\\.running_(mean|var)$', k):\n",
    "            del resumed_state_dict[k]\n",
    "    model.load_state_dict(resumed_state_dict)\n",
    "#     model, compression_scheduler, start_epoch = apputils.load_checkpoint(\n",
    "#         model, chkpt_file=args.resume)\n",
    "    \n",
    "# Define loss function (criterion) and optimizer\n",
    "optimizer = Adam(model.parameters(), args.lr)\n",
    "msglogger.info('Optimizer Type: %s', type(optimizer))\n",
    "msglogger.info('Optimizer Args: %s', optimizer.defaults)\n",
    "\n",
    "# This sample application can be invoked to produce various summary reports.\n",
    "# if args.summary:\n",
    "    # return summarize_model(model, args.dataset, which_summary=args.summary)\n",
    "    \n",
    "# Load the datasets: the dataset to load is inferred from the model name passed\n",
    "# in args.arch.  The default dataset is ImageNet, but if args.arch contains the\n",
    "# substring \"_cifar\", then cifar10 is used.\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(args.image_size),\n",
    "    transforms.CenterCrop(args.image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.mul(255))\n",
    "])\n",
    "train_dataset = datasets.ImageFolder(args.dataset, transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size)\n",
    "msglogger.info('Dataset sizes:\\n\\ttraining=%d\\n',\n",
    "               len(train_loader.sampler))\n",
    "\n",
    "if args.compress:\n",
    "    # The main use-case for this sample application is CNN compression. Compression\n",
    "    # requires a compression schedule configuration file in YAML.\n",
    "    compression_scheduler = distiller.file_config(model, optimizer, args.compress)\n",
    "#     # Model is re-transferred to GPU in case parameters were added (e.g. PACTQuantizer)\n",
    "#     model.cuda()\n",
    "    model.to(device)\n",
    "else:\n",
    "    compression_scheduler = distiller.CompressionScheduler(model)\n",
    "        \n",
    "for epoch in range(start_epoch, start_epoch + args.epochs):\n",
    "    # This is the main training loop.\n",
    "    msglogger.info('\\n')\n",
    "    if compression_scheduler:\n",
    "        compression_scheduler.on_epoch_begin(epoch)\n",
    "        \n",
    "        # Train for one epoch\n",
    "        train(train_loader, model, optimizer, vgg, epoch, compression_scheduler, [tflogger, pylogger],\n",
    "              args.log_interval, args.style_image, args.style_size, args.content_weight, args.style_weight)\n",
    "        distiller.log_weights_sparsity(model, epoch, loggers=[tflogger, pylogger])\n",
    "        if args.activation_stats:\n",
    "            distiller.log_activation_sparsity(epoch, loggers=[tflogger, pylogger],\n",
    "                                              collector=activations_sparsity)\n",
    "\n",
    "        if compression_scheduler:\n",
    "            compression_scheduler.on_epoch_end(epoch, optimizer)\n",
    "\n",
    "        # remember best top1 and save checkpoint\n",
    "        is_best = top1 > best_top1\n",
    "        if is_best:\n",
    "            best_epoch = epoch\n",
    "            best_top1 = top1\n",
    "        msglogger.info('==> Best Top1: %.3f   On Epoch: %d\\n', best_top1, best_epoch)\n",
    "        apputils.save_checkpoint(epoch, args.arch, model, optimizer, compression_scheduler, best_top1, is_best,\n",
    "                                 args.name, msglogger.logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERALL_LOSS_KEY = 'Overall Loss'\n",
    "OBJECTIVE_LOSS_KEY = 'Objective Loss'\n",
    "\n",
    "def train(train_loader, model, optimizer, vgg, epoch, compression_scheduler, loggers,\n",
    "          log_interval, style_image, style_size, content_weight, style_weight):\n",
    "#     np.random.seed(args.seed)\n",
    "#     torch.manual_seed(args.seed)\n",
    "    \"\"\"Training loop for one epoch.\"\"\"\n",
    "    losses = OrderedDict([(OVERALL_LOSS_KEY, tnt.AverageValueMeter()),\n",
    "                          (OBJECTIVE_LOSS_KEY, tnt.AverageValueMeter())])\n",
    "   \n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "    \n",
    "    total_samples = len(train_loader.sampler)\n",
    "    batch_size = train_loader.batch_size\n",
    "    steps_per_epoch = math.ceil(total_samples / batch_size)\n",
    "    msglogger.info('Training epoch: %d samples (%d per mini-batch)', total_samples, batch_size)\n",
    "    \n",
    "    style = utils.load_image(style_image, size=style_size)\n",
    "    style = style_transform(style)\n",
    "    style = style.repeat(batch_size, 1, 1, 1).to(device)\n",
    "    \n",
    "    features_style = vgg(utils.normalize_batch(style))\n",
    "    gram_style = [utils.gram_matrix(y) for y in features_style]\n",
    "    \n",
    "    model.train()\n",
    "    agg_content_loss = 0.\n",
    "    agg_style_loss = 0.\n",
    "    count = 0\n",
    "    end = time.time()\n",
    "    for batch_id, (x, _) in enumerate(train_loader):\n",
    "        n_batch = len(x)\n",
    "        count += n_batch\n",
    "        \n",
    "        # Execute the forward phase, compute the output and measure loss\n",
    "        if compression_scheduler:\n",
    "            compression_scheduler.on_minibatch_begin(epoch, batch_id, steps_per_epoch, optimizer)\n",
    "\n",
    "        x = x.to(device)\n",
    "        y = model(x)\n",
    "\n",
    "        y = utils.normalize_batch(y)\n",
    "        x = utils.normalize_batch(x)\n",
    "\n",
    "        features_y = vgg(y)\n",
    "        features_x = vgg(x)\n",
    "\n",
    "        content_loss = content_weight * mse_loss(features_y.relu2_2, features_x.relu2_2)\n",
    "\n",
    "        style_loss = 0.\n",
    "        for ft_y, gm_s in zip(features_y, gram_style):\n",
    "            gm_y = utils.gram_matrix(ft_y)\n",
    "            style_loss += mse_loss(gm_y, gm_s[:n_batch, :, :])\n",
    "        style_loss *= style_weight\n",
    "\n",
    "        loss = content_loss + style_loss\n",
    "\n",
    "        losses[OBJECTIVE_LOSS_KEY].add(loss.item())\n",
    "        \n",
    "        if compression_scheduler:\n",
    "            # Before running the backward phase, we allow the scheduler to modify the loss\n",
    "            # (e.g. add regularization loss)\n",
    "            agg_loss = compression_scheduler.before_backward_pass(epoch, batch_id, steps_per_epoch, loss,\n",
    "                                                                  optimizer=optimizer, return_loss_components=True)\n",
    "            loss = agg_loss.overall_loss\n",
    "            losses[OVERALL_LOSS_KEY].add(loss.item())\n",
    "            for lc in agg_loss.loss_components:\n",
    "                if lc.name not in losses:\n",
    "                    losses[lc.name] = tnt.AverageValueMeter()\n",
    "                losses[lc.name].add(lc.value.item())\n",
    "        \n",
    "        # Compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if compression_scheduler:\n",
    "            compression_scheduler.on_minibatch_end(epoch, batch_id, steps_per_epoch, optimizer)\n",
    "        \n",
    "        \n",
    "        if (batch_id + 1) % log_interval == 0:\n",
    "            stats_dict = OrderedDict()\n",
    "            for loss_name, meter in losses.items():\n",
    "                stats_dict[loss_name] = meter.mean\n",
    "            # stats_dict.update(errs)\n",
    "            stats_dict['LR'] = optimizer.param_groups[0]['lr']\n",
    "            # stats_dict['Time'] = batch_time.mean\n",
    "            stats = ('Peformance/Training/', stats_dict)\n",
    "\n",
    "            params = model.named_parameters() if args.log_params_histograms else None\n",
    "            distiller.log_training_progress(stats,\n",
    "                                            params,\n",
    "                                            epoch, batch_id+1,\n",
    "                                            steps_per_epoch, log_interval,\n",
    "                                            loggers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, loggers, args, epoch=-1):\n",
    "    \"\"\"Model validation\"\"\"\n",
    "    if epoch > -1:\n",
    "        msglogger.info('--- validate (epoch=%d)-----------', epoch)\n",
    "    else:\n",
    "        msglogger.info('--- validate ---------------------')\n",
    "    return _validate(val_loader, model, criterion, loggers, args, epoch)\n",
    "\n",
    "def test(test_loader, model, criterion, loggers, args):\n",
    "    \"\"\"Model Test\"\"\"\n",
    "    msglogger.info('--- test ---------------------')\n",
    "    return _validate(test_loader, model, criterion, loggers, args)\n",
    "\n",
    "def _validate(data_loader, model, criterion, loggers, args, epoch=-1):\n",
    "    \"\"\"Execute the validation/test loop.\"\"\"\n",
    "    losses = {'objective_loss': tnt.AverageValueMeter()}\n",
    "    classerr = tnt.ClassErrorMeter(accuracy=True, topk=(1, 5))\n",
    "\n",
    "    if args.earlyexit_thresholds:\n",
    "        # for Early Exit, we have a list of errors and losses for each of the exits.\n",
    "        args.exiterrors = []\n",
    "        args.losses_exits = []\n",
    "        for exitnum in range(args.num_exits):\n",
    "            args.exiterrors.append(tnt.ClassErrorMeter(accuracy=True, topk=(1, 5)))\n",
    "            args.losses_exits.append(tnt.AverageValueMeter())\n",
    "        args.exit_taken = [0] * args.num_exits\n",
    "\n",
    "    batch_time = tnt.AverageValueMeter()\n",
    "    total_samples = len(data_loader.sampler)\n",
    "    batch_size = data_loader.batch_size\n",
    "    if args.display_confusion:\n",
    "        confusion = tnt.ConfusionMeter(args.num_classes)\n",
    "    total_steps = total_samples / batch_size\n",
    "    msglogger.info('%d samples (%d per mini-batch)', total_samples, batch_size)\n",
    "\n",
    "    # Switch to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for validation_step, (inputs, target) in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            inputs, target = inputs.to('cuda'), target.to('cuda')\n",
    "            # compute output from model\n",
    "            output = model(inputs)\n",
    "\n",
    "            if not args.earlyexit_thresholds:\n",
    "                # compute loss\n",
    "                loss = criterion(output, target)\n",
    "                # measure accuracy and record loss\n",
    "                losses['objective_loss'].add(loss.item())\n",
    "                classerr.add(output.data, target)\n",
    "                if args.display_confusion:\n",
    "                    confusion.add(output.data, target)\n",
    "            else:\n",
    "                # If using Early Exit, then compute outputs at all exits - output is now a list of all exits\n",
    "                # from exit0 through exitN (i.e. [exit0, exit1, ... exitN])\n",
    "                earlyexit_validate_loss(output, target, criterion, args)\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.add(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            steps_completed = (validation_step+1)\n",
    "            if steps_completed % args.print_freq == 0:\n",
    "                if not args.earlyexit_thresholds:\n",
    "                    stats = ('',\n",
    "                            OrderedDict([('Loss', losses['objective_loss'].mean),\n",
    "                                         ('Top1', classerr.value(1)),\n",
    "                                         ('Top5', classerr.value(5))]))\n",
    "                else:\n",
    "                    stats_dict = OrderedDict()\n",
    "                    stats_dict['Test'] = validation_step\n",
    "                    for exitnum in range(args.num_exits):\n",
    "                        la_string = 'LossAvg' + str(exitnum)\n",
    "                        stats_dict[la_string] = args.losses_exits[exitnum].mean\n",
    "                        # Because of the nature of ClassErrorMeter, if an exit is never taken during the batch,\n",
    "                        # then accessing the value(k) will cause a divide by zero. So we'll build the OrderedDict\n",
    "                        # accordingly and we will not print for an exit error when that exit is never taken.\n",
    "                        if args.exit_taken[exitnum]:\n",
    "                            t1 = 'Top1_exit' + str(exitnum)\n",
    "                            t5 = 'Top5_exit' + str(exitnum)\n",
    "                            stats_dict[t1] = args.exiterrors[exitnum].value(1)\n",
    "                            stats_dict[t5] = args.exiterrors[exitnum].value(5)\n",
    "                    stats = ('Performance/Validation/', stats_dict)\n",
    "\n",
    "                distiller.log_training_progress(stats, None, epoch, steps_completed,\n",
    "                                                total_steps, args.print_freq, loggers)\n",
    "    if not args.earlyexit_thresholds:\n",
    "        msglogger.info('==> Top1: %.3f    Top5: %.3f    Loss: %.3f\\n',\n",
    "                       classerr.value()[0], classerr.value()[1], losses['objective_loss'].mean)\n",
    "\n",
    "        if args.display_confusion:\n",
    "            msglogger.info('==> Confusion:\\n%s\\n', str(confusion.value()))\n",
    "        return classerr.value(1), classerr.value(5), losses['objective_loss'].mean\n",
    "    else:\n",
    "        # Print some interesting summary stats for number of data points that could exit early\n",
    "        top1k_stats = [0] * args.num_exits\n",
    "        top5k_stats = [0] * args.num_exits\n",
    "        losses_exits_stats = [0] * args.num_exits\n",
    "        sum_exit_stats = 0\n",
    "        for exitnum in range(args.num_exits):\n",
    "            if args.exit_taken[exitnum]:\n",
    "                sum_exit_stats += args.exit_taken[exitnum]\n",
    "                msglogger.info(\"Exit %d: %d\", exitnum, args.exit_taken[exitnum])\n",
    "                top1k_stats[exitnum] += args.exiterrors[exitnum].value(1)\n",
    "                top5k_stats[exitnum] += args.exiterrors[exitnum].value(5)\n",
    "                losses_exits_stats[exitnum] += args.losses_exits[exitnum].mean\n",
    "        for exitnum in range(args.num_exits):\n",
    "            if args.exit_taken[exitnum]:\n",
    "                msglogger.info(\"Percent Early Exit %d: %.3f\", exitnum,\n",
    "                               (args.exit_taken[exitnum]*100.0) / sum_exit_stats)\n",
    "\n",
    "        return top1k_stats[args.num_exits-1], top5k_stats[args.num_exits-1], losses_exits_stats[args.num_exits-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the gradual sparsity function\n",
    "\n",
    "The function ```sparsity_target``` implements the gradual sparsity schedule from [[1]](#zhu-gupta):<br><br>\n",
    "<b><i>\"We introduce a new automated gradual pruning algorithm in which the sparsity is increased from an initial sparsity value $s_i$ (usually 0) to a final sparsity value $s_f$ over a span of $n$ pruning steps, starting at training step $t_0$ and with pruning frequency $\\Delta t$.\"</i></b><br>\n",
    "<br>\n",
    "\n",
    "<div id=\"eq:zhu_gupta_schedule\"></div>\n",
    "<center>\n",
    "$\\large\n",
    "\\begin{align}\n",
    "s_t = s_f + (s_i - s_f) \\left(1- \\frac{t-t_0}{n\\Delta t}\\right)^3\n",
    "\\end{align}\n",
    "\\ \\ for\n",
    "\\large \\ \\ t \\in \\{t_0, t_0+\\Delta t, ..., t_0+n\\Delta t\\}\n",
    "$\n",
    "</center>\n",
    "<br>\n",
    "Pruning happens once at the beginning of each epoch, until the duration of the pruning (the number of epochs to prune) is exceeded.  After pruning ends, the training continues without pruning, but the pruned weights are kept at zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsity_target(starting_epoch, ending_epoch, initial_sparsity, final_sparsity, current_epoch):\n",
    "    if final_sparsity < initial_sparsity:\n",
    "        return current_epoch \n",
    "    if current_epoch < starting_epoch:\n",
    "        return current_epoch\n",
    "    \n",
    "    span = ending_epoch - starting_epoch\n",
    "    target_sparsity = ( final_sparsity +\n",
    "                        (initial_sparsity - final_sparsity) *\n",
    "                        (1.0 - ((current_epoch-starting_epoch)/span))**3)\n",
    "    return target_sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize pruning schedule\n",
    "When using the Automated Gradual Pruning (AGP) schedule, you may want to visualize how the pruning schedule will look as a function of the epoch number.  This is called the *sparsity function*.  The widget below will help you do this.<br>\n",
    "There are three knobs you can use to change the schedule:\n",
    "- ```duration```: this is the number of epochs over which to use the AGP schedule ($n\\Delta t$).\n",
    "- ```initial_sparsity```: $s_i$\n",
    "- ```final_sparsity```: $s_f$\n",
    "- ```frequency```: this is the pruning frequency ($\\Delta t$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pruning(duration, initial_sparsity, final_sparsity, frequency):\n",
    "    epochs = []\n",
    "    sparsity_levels = []\n",
    "    # The derivative of the sparsity (i.e. sparsity rate of change)\n",
    "    d_sparsity = []\n",
    "\n",
    "    if frequency=='':\n",
    "        frequency = 1 \n",
    "    else:\n",
    "        frequency = int(frequency)\n",
    "    for epoch in range(0,40):\n",
    "        epochs.append(epoch)\n",
    "        current_epoch=Variable(torch.FloatTensor([epoch]), requires_grad=True)\n",
    "        if epoch<duration and epoch%frequency == 0:\n",
    "            sparsity = sparsity_target(\n",
    "                     starting_epoch=0, \n",
    "                     ending_epoch=duration, \n",
    "                     initial_sparsity=initial_sparsity, \n",
    "                     final_sparsity=final_sparsity,\n",
    "                current_epoch=current_epoch\n",
    "            )\n",
    "            \n",
    "            sparsity_levels.append(sparsity)\n",
    "            sparsity.backward()\n",
    "            d_sparsity.append(current_epoch.grad.item())\n",
    "            current_epoch.grad.data.zero_()\n",
    "        else:\n",
    "            sparsity_levels.append(sparsity)\n",
    "            d_sparsity.append(0)\n",
    "            \n",
    "\n",
    "    plt.plot(epochs, sparsity_levels, epochs, d_sparsity)\n",
    "    plt.ylabel('sparsity (%)')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.title('Pruning Rate')\n",
    "    plt.ylim(0, 100)\n",
    "    plt.draw()\n",
    "\n",
    "\n",
    "duration_widget = widgets.IntSlider(min=0, max=100, step=1, value=28)\n",
    "si_widget = widgets.IntSlider(min=0, max=100, step=1, value=0)\n",
    "interact(draw_pruning, \n",
    "         duration=duration_widget, \n",
    "         initial_sparsity=si_widget, \n",
    "         final_sparsity=(0,100,1),\n",
    "         frequency='2'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"toc\"></div>\n",
    "## References\n",
    "1. <div id=\"zhu-gupta\"></div> **Michael Zhu and Suyog Gupta**. \n",
    "    [*To prune, or not to prune: exploring the efficacy of pruning for model compression*](https://arxiv.org/pdf/1710.01878),\n",
    "    NIPS Workshop on Machine Learning of Phones and other Consumer Devices,\n",
    "    2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
